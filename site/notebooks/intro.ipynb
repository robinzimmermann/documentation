{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValidMind Python Library Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python. \n",
    "\n",
    "For this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\n",
    "\n",
    "We will train a sample model and demonstrate the following documentation functionalities:\n",
    "\n",
    "- Logging information about a dataset\n",
    "- Running data quality tests on a dataset\n",
    "- Logging information about a model\n",
    "- Logging training metrics for a model\n",
    "- Running model evaluation tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Example Model\n",
    "We will now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we'll train a model for the Bank Customer Churn dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading demo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/bank_customer_churn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training dataset\n",
    "\n",
    "Before we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n",
    "\n",
    "- Dropping irrelevant variables\n",
    "- Encoding categorical variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant variables\n",
    "\n",
    "The following variables will be dropped from the dataset:\n",
    "\n",
    "- `RowNumber`: it's a unique identifier to the record\n",
    "- `CustomerId`: it's a unique identifier to the customer\n",
    "- `Surname`: no predictive power for this variable\n",
    "- `CreditScore`: we didn't observer any correlation between `CreditScore` and our target column `Exited`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "\n",
    "We will apply one-hot or dummy encoding to the following variables:\n",
    "\n",
    "- `Geography`: only 3 unique values found in the dataset\n",
    "- `Gender`: convert from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "df.replace({\"Gender\": genders}, inplace=True)\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "df.drop(\"Geography\", axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model with the preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "For training our model, we will **randomly** split the dataset in 3 parts:\n",
    "\n",
    "- `training` split with 60% of the rows\n",
    "- `validation` split with 20% of the rows\n",
    "- `test` split with 20% of the rows\n",
    "\n",
    "The `test` dataset will be our held out dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "# This guarantees a 60/20/20 split\n",
    "train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "# For training\n",
    "x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "# For testing\n",
    "x_test = test_df.drop(\"Exited\", axis=1)\n",
    "y_test = test_df.loc[:, \"Exited\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We will train a simple XGBoost model and set its `eval_set` to `[(x_train, y_train), (x_val, y_val)]` in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of \"in training\" metrics so model developers can provide additional context to model validators if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(x_val)[:, -1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ValidMind Library\n",
    "\n",
    "Log in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n",
    "\n",
    "### Creating a new Documentation Project \n",
    "\n",
    "***(Note: if a documentation project has already been created, you can skip this section and head directly to the next)***\n",
    "\n",
    "Clicking on \"Create a new project\" allows to you to register a new documentation project for our demo model. \n",
    "\n",
    "Select \"Customer Churn model\" from the Model drop-down, and \"Initial Validation\" as Type. Finally, click on \"Create Project\".\n",
    "\n",
    "### Finding the project API key and secret \n",
    "\n",
    "In the \"Client Integration\" page of the newly created project, you can now find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments: \n",
    "\n",
    "* api_host: Location of the ValidMind API.\n",
    "* api_key: Account API key.\n",
    "* api_secret: Account Secret key.\n",
    "* project: The project identifier. The `project` argument is mandatory since it allows the library to associate all data collected with a specific account project.\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/settings.png\" width=\"600\" height=\"300\" />\n",
    "\n",
    "The code snippet can be copied and pasted directly into your developer source code and initialize the ValidMind Developer Framework when run:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"https://api.staging.validmind.ai/api/v1/tracking\",\n",
    "  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n",
    "  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n",
    "  project = \"cl1jyv16o000809lg98gi9tie\"\n",
    ")\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Developer Framework is now initialized and connected to the correct project on the platform. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing all test plans available in the developer framework\n",
    "\n",
    "We can find all the test plans and tests available in the developer framework by calling the following functions:\n",
    "\n",
    "- All test plans: `vm.test_plans.list_plans()`\n",
    "- Describe a test plan: `vm.test_plans.describe_plan(\"tabular_dataset\")`\n",
    "- List all available tests: `vm.test_plans.list_tests()`\n",
    "\n",
    "As an example, here's the output `list_plans()` and `list_tests()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.list_plans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.list_tests()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a data quality test plan\n",
    "\n",
    "We will now run the default data quality test plan that will collect the\n",
    "following metadata from a dataset:\n",
    "\n",
    "- Field types and descriptions\n",
    "- Descriptive statistics\n",
    "- Data distribution histograms\n",
    "- Feature correlations\n",
    "\n",
    "and will run a collection of data quality tests such as:\n",
    "\n",
    "- Class imbalance\n",
    "- Duplicates\n",
    "- High cardinality\n",
    "- Missing values\n",
    "- Skewness\n",
    "\n",
    "ValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n",
    "\n",
    "#### Load the demo dataset\n",
    "\n",
    "Before running the test plan, we must first initialize\n",
    "a ValidMind dataset object using the `init_dataset` function from the `vm` module. This function takes in arguements: `dataset` which is the dataset that we want to analyze; `target_column` which is used to identify the target variable; `class_labels` which is used to identify the labels used for classification model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and run the TabularDataset test plan\n",
    "\n",
    "We can now initialize the `TabularDataset` test plan. The primary method of doing this is with the `run_test_plan` function from the `vm` module. This function takes in a test plan name (in this case `tabular_dataset`) and a `dataset` keyword argument (the `vm_dataset` object we created earlier):\n",
    "\n",
    "```python\n",
    "vm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a model evaluation test plan\n",
    "\n",
    "We will now run a basic model evaluation test plan that is compatible with the model we have trained.\n",
    "Since we have trained an XGBoost model with a sklearn-like API, we will use the `SKLearnClassifier` test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\n",
    "\n",
    "The following model metadata is collected:\n",
    "\n",
    "- Model framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\n",
    "- Model task details (e.g. binary classification, regression, etc.)\n",
    "- Model hyperparameters (e.g. number of trees, max depth, etc.)\n",
    "\n",
    "The model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n",
    "\n",
    "- AUC\n",
    "- Error rate\n",
    "- Logloss\n",
    "- Feature importance\n",
    "\n",
    "Similarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n",
    "\n",
    "- Simple training/test overfit test\n",
    "- Training/test performance degradation\n",
    "- Baseline test dataset performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize VM model object and train/test datasets\n",
    "\n",
    "In order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(model)\n",
    "vm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\n",
    "vm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the `SKLearnClassifier` test plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "804f302cc8b7478038da7bdcfb2808de81068729a5926c8fae893a478342a1b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
