[
  {
    "objectID": "guide/getstartedvalidator.html",
    "href": "guide/getstartedvalidator.html",
    "title": "Get started as a validator",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "guide/about.html",
    "href": "guide/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "guide/getstarteddeveloper.html",
    "href": "guide/getstarteddeveloper.html",
    "title": "Get started as a developer",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "guide/getstartedauditor.html",
    "href": "guide/getstartedauditor.html",
    "title": "Get started as an auditor",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset(dataset, type=‘training’, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model(model)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan(test_plan_name, send=True, **kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘sklearn_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metadata(content_id, text=None, extra_json=None)\nLogs free-form metadata to ValidMind API.\n\nParameters\n\ncontent_id (str) – Unique content identifier for the metadata\ntext (str, optional) – Free-form text to assign to the metadata. Defaults to None.\nextra_json (dict, optional) – Free-form key-value pairs to assign to the metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results(results, run_cuid=None, dataset_type=‘training’)\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset(raw_dataset: object, fields: list, variables: list, sample: list, shape: dict, correlation_matrix: ~typing.Optional[object] = None, correlations: ~typing.Optional[dict] = None, type: ~typing.Optional[str] = None, options: ~typing.Optional[dict] = None, statistics: ~typing.Optional[dict] = None, targets: ~typing.Optional[dict] = None, target_column: str = ’’, class_labels: ~typing.Optional[dict] = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: ~typing.Optional[object] = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nvariables(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets(target_column: str, description: Optional[str] = None, class_labels: Optional[dict] = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure(key: str, metadata: dict, figure: object)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric(test_context: TestContext, params: Optional[dict] = None, result: Optional[TestPlanResult] = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[Optional[str] _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: Union[dict, list, DataFrame], figures: Optional[object] = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model(attributes: Optional[ModelAttributes] = None, task: Optional[str] = None, subtask: Optional[str] = None, params: Optional[dict] = None, model_id: str = ‘main’, model: Optional[object] = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nclassmethod is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes(architecture: Optional[str] = None, framework: Optional[str] = None, framework_version: Optional[str] = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult(*, test_name: Optional[str] = None, column: Optional[str] = None, passed: Optional[bool] = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: Optional[str )\n\n\ncolumn(: Optional[str )\n\n\npassed(: Optional[bool )\n\n\nvalues(: dic )\n\n\n\nclass validmind.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.ThresholdTest(test_context: TestContext, params: Optional[dict] = None, test_results: Optional[TestResults] = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset(raw_dataset: object, fields: list, variables: list, sample: list, shape: dict, correlation_matrix: ~typing.Optional[object] = None, correlations: ~typing.Optional[dict] = None, type: ~typing.Optional[str] = None, options: ~typing.Optional[dict] = None, statistics: ~typing.Optional[dict] = None, targets: ~typing.Optional[dict] = None, target_column: str = ’’, class_labels: ~typing.Optional[dict] = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: ~typing.Optional[object] = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nvariables(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets(target_column: str, description: Optional[str] = None, class_labels: Optional[dict] = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure(key: str, metadata: dict, figure: object)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric(test_context: TestContext, params: Optional[dict] = None, result: Optional[TestPlanResult] = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[Optional[str] _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: Union[dict, list, DataFrame], figures: Optional[object] = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.vm_models.MetricResult(type: str, scope: str, key: dict, value: Union[dict, list, DataFrame], value_formatter: Optional[str] = None)\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: Union[dict, list, DataFrame )\n\n\nvalue_formatter(: Optional[str _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model(attributes: Optional[ModelAttributes] = None, task: Optional[str] = None, subtask: Optional[str] = None, params: Optional[dict] = None, model_id: str = ‘main’, model: Optional[object] = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nclassmethod is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes(architecture: Optional[str] = None, framework: Optional[str] = None, framework_version: Optional[str] = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContext(dataset: Optional[Dataset] = None, model: Optional[Model] = None, train_ds: Optional[Dataset] = None, test_ds: Optional[Dataset] = None, y_train_predict: Optional[object] = None, y_test_predict: Optional[object] = None)\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as model predictions, shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContextUtils()\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty train_ds()\n\n\nproperty test_ds()\n\n\nproperty y_train_predict()\n\n\nproperty y_test_predict()\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\n\nclass validmind.vm_models.TestPlan(config: {} = None, test_context: TestContext = None, dataset: Dataset = None, model: Model = None, train_ds: Dataset = None, test_ds: Dataset = None)\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[object] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\n\n\n\nclass validmind.vm_models.TestPlanResult(dataset: Optional[object] = None, metric: Optional[object] = None, model: Optional[object] = None, test_results: Optional[object] = None, figures: Optional[object] = None, plots: Optional[List[object]] = None)\nBases: object\nResult wrapper tests that run as part of a test plan\n\ndataset(: Optional[object _ = Non_ )\n\n\nmetric(: Optional[object _ = Non_ )\n\n\nmodel(: Optional[object _ = Non_ )\n\n\ntest_results(: Optional[object _ = Non_ )\n\n\nfigures(: Optional[object _ = Non_ )\n\n\nplots(: Optional[List[object] _ = Non_ )\n\n\n\nclass validmind.vm_models.TestResult(*, test_name: Optional[str] = None, column: Optional[str] = None, passed: Optional[bool] = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: Optional[str )\n\n\ncolumn(: Optional[str )\n\n\npassed(: Optional[bool )\n\n\nvalues(: dic )\n\n\n\nclass validmind.vm_models.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.vm_models.ThresholdTest(test_context: TestContext, params: Optional[dict] = None, test_results: Optional[TestResults] = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: TestContextUtils\nSHAP Global Importance. Custom metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the accuracy score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the F1 score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the ROC AUC score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the training set metrics are better than the test set metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "Ensure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=prerelease\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate Markdown docs\nmake markdown\n\n# Generate HTML docs\nmake html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nTest plan for sklearn classifier models\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan for sklearn classifier metrics\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models that includes both metrics and validation tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for generic tabular datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadat is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the minority class does not represent more than a threshold of the total number of examples\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of duplicates is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique values in a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the Pearson correlation between two columns is less than a threshold\nInspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the skewness of a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of zeros is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Find all the information you need to use our platform for model risk management (MRM).\nGet started as a developer\nGet started as a validator\nGet started as an auditor"
  },
  {
    "objectID": "notebooks/lending_club.html",
    "href": "notebooks/lending_club.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the library code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local library code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\n# Use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\" if you want to connect to the dev environment\nvm.init(\n    project=\"cl1jyv16o000809lg98gi9tie\"\n)\n\nTrue\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\nBefore we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n\n\n\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured content_id can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the Model Overview section for a project, they can use model_overview as the content_id:\nvm.log_metadata(\"model_overview\", text=\"Testing\")\nThe text argument accepts Markdown formatted text as we’ll see in the cell below. The documentation used for this model has been taken from the Kaggle dataset.\n\nmodel_overview = \"\"\"\nTesting writing metadata from the framework\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nThe dashboard should now display the Model Overview section with the text we have provided from the library:\n\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataQuality test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_data_quality) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nRunning test plan 'tabular_data_quality'...\n\n\n\n\n\nRunning ThresholdTest: class_imbalance\nRunning ThresholdTest: duplicates\nRunning ThresholdTest: cardinality\nRunning ThresholdTest: pearson_correlation\nRunning ThresholdTest: missing\nRunning ThresholdTest: skewness\nRunning ThresholdTest: unique\nRunning ThresholdTest: zeros\nSending results of test plan execution 'tabular_data_quality' to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: unique\nSuccessfully logged test results for test: zeros\n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                           Name                        Description                                   \n\n\nsklearn_classifier_metrics   SKLearnClassifierMetrics    Test plan for sklearn classifier metrics      \nsklearn_classifier_validationSKLearnClassifierPerformanceTest plan for sklearn classifier models       \nsklearn_classifier           SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                               \ntabular_dataset              TabularDataset              Test plan for generic tabular datasets        \ntabular_dataset_description  TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                               \ntabular_data_quality         TabularDataQuality          Test plan for data quality on tabular datasets\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                       Name                        Description                                                               \n\n\nMetric       dataset_correlations     DatasetCorrelations         Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                           \nMetric       dataset_description      DatasetDescription          Collects a set of descriptive statistics for a dataset                    \nCustom Test  dataset_metadata         DatasetMetadata             Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                           \nThresholdTestclass_imbalance          ClassImbalanceTest          Test that the minority class does not represent more than a threshold\n    of the total number of examples                                                                           \nThresholdTestduplicates               DuplicatesTest              Test that the number of duplicates is less than a threshold               \nThresholdTestcardinality              HighCardinalityTest         Test that the number of unique values in a column is less than a threshold\nThresholdTestpearson_correlation      HighPearsonCorrelationTest  Test that the Pearson correlation between two columns is less than a threshold\n\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           \nThresholdTestmissing                  MissingValuesTest           Test that the number of missing values is less than a threshold           \nThresholdTestskewness                 SkewnessTest                Test that the skewness of a column is less than a threshold               \nThresholdTestunique                   UniqueRowsTest              Test that the number of unique rows is greater than a threshold           \nThresholdTestzeros                    ZerosTest                   Test that the number of zeros is less than a threshold                    \nMetric       accuracy                 AccuracyScore               Accuracy Score                                                            \nMetric       csi                      CharacteristicStabilityIndexCharacteristic Stability Index between two datasets                       \nMetric       confusion_matrix         ConfusionMatrix             Confusion Matrix                                                          \nMetric       f1_score                 F1Score                     F1 Score                                                                  \nMetric       pfi                      PermutationFeatureImportancePermutation Feature Importance                                            \nMetric       psi                      PopulationStabilityIndex    Population Stability Index between two datasets                           \nMetric       pr_curve                 PrecisionRecallCurve        Precision Recall Curve                                                    \nMetric       precision                PrecisionScore              Precision Score                                                           \nMetric       roc_auc                  ROCAUCScore                 ROC AUC Score                                                             \nMetric       roc_curve                ROCCurve                    ROC Curve                                                                 \nMetric       recall                   RecallScore                 Recall Score                                                              \nCustom Test  shap                     SHAPGlobalImportance        SHAP Global Importance. Custom metric                                     \nThresholdTestaccuracy_score           AccuracyTest                Test that the accuracy score is above a threshold.                        \nThresholdTestf1_score                 F1ScoreTest                 Test that the F1 score is above a threshold.                              \nThresholdTestroc_auc_score            ROCAUCScoreTest             Test that the ROC AUC score is above a threshold.                         \nThresholdTesttraining_test_degradationTrainingTestDegradationTest Test that the training set metrics are better than the test set metrics.  \n\n\n\n\nOnce the TabularDataset test plan has finished running, we can view the results in the ValidMind dashboard:\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.865625\n\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning test plan 'sklearn_classifier'...\nGenerating predictions train dataset...\nGenerating predictions test dataset...\n\n\n\n\n\nSending results of test plan execution 'sklearn_classifier' to ValidMind...\n|-- Running sub test plan - sklearn_classifier_metrics\nRunning test plan 'sklearn_classifier_metrics'...\n\n\n\n\n\nRunning ModelMetadata: model_metadata\nRunning Metric: accuracy\nRunning Metric: confusion_matrix\nRunning Metric: f1_score\nRunning Metric: pfi\nRunning Metric: pr_curve\nRunning Metric: precision\nRunning Metric: recall\nRunning Metric: roc_auc\nRunning Metric: roc_curve\nRunning Metric: csi\nRunning Metric: psi\nRunning SHAPGlobalImportance: shap\n\n\nntree_limit is deprecated, use `iteration_range` or model slicing instead.\n\n\nSending results of test plan execution 'sklearn_classifier_metrics' to ValidMind...\nSuccessfully logged metrics\n|-- Running sub test plan - sklearn_classifier_validation\nRunning test plan 'sklearn_classifier_validation'...\n\n\n\n\n\nRunning ThresholdTest: accuracy_score\nRunning ThresholdTest: f1_score\nRunning ThresholdTest: roc_auc_score\nRunning ThresholdTest: training_test_degradation\nSending results of test plan execution 'sklearn_classifier_validation' to ValidMind...\nSuccessfully logged test results for test: accuracy_score\nSuccessfully logged test results for test: f1_score\nSuccessfully logged test results for test: roc_auc_score\nSuccessfully logged test results for test: training_test_degradation"
  },
  {
    "objectID": "notebooks/library_intro_demos.html",
    "href": "notebooks/library_intro_demos.html",
    "title": "ValidMind",
    "section": "",
    "text": "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle.\nFor modelers, the client provides the following high level features:\n\nLog qualitative data about the model’s conceptual soundness\nLog information about datasets and models\nLog training and evaluation metrics about datasets and models\nRun data quality checks\nRun model evaluation tests\n\nFor validators, the client also provides (TBD) the ability to effectively challenge the model’s performance according to its objective, use case and specific project’s requirements.\n\n\n\nThis notebook and the ValidMind client must be executed on an environment running Python >= 3.8.\n\n\n\n\nWhile we finish the process of making the library publicly accessible pip, it can be installed with the following command that will direct pip to the S3 bucket that contains the latest version of the client.\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv('./env')\n\nTrue\n\n\n\n\n\n\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\n\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_metadata\nLogs free-form metadata text for a given content ID in the model documentation\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\n\nLogs free-form metadata text for a given content ID in the model documentation.\nArguments:\n\ncontent_id: Content ID of the model documentation. This is a unique identifier generated by the ValidMind dashboard. See available content_ids in the model training section below\ntext: Free-form text to be logged. A text template can be specified in combination with extra_json (see below)\nextra_json: (TBD support for this) JSON object containing variables to be substituted in the text template\n\n\n\n\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\n\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)\n\n\n\n\n\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\n\n\nBefore we start logging information about our dataset, we’d want to send metadata to ValidMind about the model’s conceptual soundness, for example. Model developers have the option to directly populate parts of the dashboard documentation using special content_ids. The following is the list of content_ids supported at the moment:\n\n\n\n\n\n\n\nContent ID\nPopulates Section\n\n\n\n\nmodel_overview\nConceptual Soundness -> Model Overview\n\n\nmodel_selection\nConceptual Soundness -> Model Selection\n\n\nbusiness_case\nConceptual Soundness -> Intended Use and Business Use Case\n\n\nfeature_selection\nData Preparation -> Feature Selection and Engineering\n\n\ngovernance_plan\nMonitoring and Governance -> Governance Plan\n\n\nmonitoring_implementation\nMonitoring and Governance -> Monitoring Implementation\n\n\nmonitoring_plan\nMonitoring and Governance -> Monitoring Plan\n\n\n\nIn the following log_metadata example, we will populate the Model Overview section in the dashboard:\n\nmodel_overview = \"\"\"\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\n\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\n\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\n\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/lending_club_regression.html",
    "href": "notebooks/lending_club_regression.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/explore_x_train_lc.html",
    "href": "notebooks/explore_x_train_lc.html",
    "title": "ValidMind",
    "section": "",
    "text": "Explore x to train LC\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# PD Model\nvm.init(project=\"cl1jyvh2c000909lg1rk0a0zb\")\n\nTrue\n\n\n\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\n\ndef jeffreys_test(p: float, n: int = 0, d: int = 0) -> float:\n    \"\"\"\n    Perform a test that the test probability, p, is consistent with the observed number of \n    successes, d, from a number of trials, n.\n\n    This uses the Jeffrey's posterior probability, which is the Beta distribution with shape\n    parameters a = d + 1/2 and b = n - d + 1/2. The result is the one sided p-value representing the \n    probability that the test probability, p, is greater than the true probability.\n\n    :param p: the test probability to be compared to the number of successes given n trials\n    :param n: the number of trials\n    :param d: the number of successes [optional, default = 0]\n\n    :return p-value: one sided p-value of the test\n    \"\"\"\n    return scipy.stats.beta.cdf(p, d + 0.5, n - d + 0.5)\n\n\ndef update_result(s, d, n, dr, p, pval, out = 'Yet to decide'):\n    return ({'Segment': s,\n            'Defaults': d,\n            'Observations': n,\n            'Default Rate': dr,\n            'Calibrated PD': p,\n            'P-value': pval, \n            'Outcome': out})\n\n\ndef calculate_and_return(df = pd.DataFrame, cal_pd = {}, pool = None, obs = 'observed', threshold = 0.9):\n    \"\"\"\n    Take the input dataframe, analyse & clean, seprate poolwise.\n    Calculate the jeffreys statistic\n    \"\"\"\n    \n    result = pd.DataFrame(columns = ['Segment', 'Defaults', 'Observations', 'Default Rate', 'Calibrated PD', 'P-value', 'Outcome'])\n    \n    n = len(df[obs])\n    d = np.sum(df[obs])\n    dr = np.round(d/n,2)\n    p = cal_pd['Model']\n    pval = np.round(jeffreys_test(p, n, d),4)\n    if pval>=threshold:\n        out = 'Satisfactory'\n    else:\n        out = 'Not Satisfactory'\n    \n    result = result.append(update_result('Model', d, n, dr, p, pval, out), ignore_index = True)\n    \n    if pool != None:\n        samples = df.groupby(pool)\n        \n        for sample in samples:\n            n = len(sample[1][obs])\n            d = np.sum(sample[1][obs])\n            dr = np.round(d/n,2)\n            p = cal_pd[sample[0]]\n            pval = np.round(jeffreys_test(p, n, d),4)\n            \n            if pval>=threshold:\n                out = 'Satisfactory'\n            else:\n                out = 'Not Satisfactory'\n            \n            result = result.append(update_result(sample[0], d, n, dr, p, pval, out), ignore_index = True)\n            \n    return result\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/x_train_lc.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      28000.0\n      7.12\n      10\n      125000.0\n      15.97\n      0.0\n      26\n      725.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      11200.0\n      10.99\n      2\n      80600.0\n      15.93\n      0.0\n      15\n      670.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      2\n      14000.0\n      15.10\n      6\n      83000.0\n      18.17\n      0.0\n      13\n      660.0\n      1.0\n      76.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      12725.0\n      12.12\n      6\n      71300.0\n      29.70\n      0.0\n      13\n      675.0\n      2.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      7200.0\n      15.31\n      1\n      25000.0\n      32.98\n      0.0\n      8\n      700.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ndf[\"acc_now_delinq\"].value_counts()\n\n0.0    59802\n1.0      187\n2.0        7\n3.0        3\n5.0        1\nName: acc_now_delinq, dtype: int64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 60.0 MB\n\n\n\ntest_df = pd.read_csv(\"./notebooks/datasets/_temp/x_test_lc.csv\")\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      15500.0\n      8.90\n      10\n      100000.0\n      0.74\n      0.0\n      14\n      715.0\n      3.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      10800.0\n      11.67\n      10\n      68000.0\n      15.44\n      1.0\n      20\n      670.0\n      1.0\n      8.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      2\n      15850.0\n      15.10\n      2\n      36000.0\n      26.50\n      0.0\n      31\n      720.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      16000.0\n      15.31\n      2\n      80000.0\n      24.54\n      1.0\n      12\n      705.0\n      0.0\n      21.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      4\n      14000.0\n      12.12\n      10\n      90000.0\n      14.07\n      0.0\n      14\n      695.0\n      1.0\n      44.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ntest_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 20.0 MB\n\n\n\nmodel = joblib.load(\"./notebooks/datasets/_temp/lc_model.pickle\")\n\n\nsegments = [\n    {\n        \"name\": \"Grade\",\n        \"segments\": [\n            {\"name\": \"Grade A\", \"query\": \"grade_A == 1\"},\n            {\"name\": \"Grade B\", \"query\": \"grade_B == 1\"},\n            {\"name\": \"Grade C\", \"query\": \"grade_C == 1\"},\n            {\"name\": \"Grade D\", \"query\": \"grade_D == 1\"},\n            {\"name\": \"Grade E\", \"query\": \"grade_E == 1\"},\n            {\"name\": \"Grade F\", \"query\": \"grade_F == 1\"},\n            {\"name\": \"Grade G\", \"query\": \"grade_G == 1\"},\n        ]\n    },\n    {\n        \"name\": \"Delinquency\",\n        \"segments\": [\n            {\"name\": \"Delinquency: None\", \"query\": \"acc_now_delinq == 0\"},\n            {\"name\": \"Delinquency: 1 Account\", \"query\": \"acc_now_delinq == 1\"},\n            {\"name\": \"Delinquency: 2 Accounts\", \"query\": \"acc_now_delinq == 2\"},\n        ]\n    }\n]\n\n\ndef get_calibrated_pds(df, model, segments):\n    model_preds = model.predict_proba(df)[:, 1]\n    model_class_preds = (model_preds > 0.5).astype(int)\n\n    pds = {\"Model\": model_class_preds.sum() / len(model_class_preds)}\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            total_pds = class_pred.sum()\n            segment_pd = total_pds / len(class_pred)\n\n            pds[segment[\"name\"]] = segment_pd\n    return pds\n\n\ncalibrated_pds = get_calibrated_pds(df, model, segments)\ncalibrated_pds\n\n{'Model': 0.027933333333333334,\n 'Grade A': 0.0022715539494062983,\n 'Grade B': 0.007202947160059383,\n 'Grade C': 0.014655226404459197,\n 'Grade D': 0.043563336766220394,\n 'Grade E': 0.10736266241167085,\n 'Grade F': 0.1781818181818182,\n 'Grade G': 0.24396135265700483,\n 'Delinquency: None': 0.027791712651750778,\n 'Delinquency: 1 Account': 0.06951871657754011,\n 'Delinquency: 2 Accounts': 0.14285714285714285}\n\n\n\ndef process_observations(df, model, segments):\n    test_input = pd.DataFrame(columns = ['Segment', 'Observed'])\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            # Concat to test_input by adding all rows of class_pred and segment as a single value\n            test_input = pd.concat([test_input, pd.DataFrame({'Segment': segment[\"name\"], 'Observed': class_pred})], ignore_index=True)\n\n    return test_input\n\n\nobservations = process_observations(test_df, model, segments)\n\n\nresults = calculate_and_return(\n    observations,\n    cal_pd=calibrated_pds,\n    pool = 'Segment',\n    obs=\n    'Observed',\n    threshold = 0.85\n)\nresults\n\n\n\n\n\n  \n    \n      \n      Segment\n      Defaults\n      Observations\n      Default Rate\n      Calibrated PD\n      P-value\n      Outcome\n    \n  \n  \n    \n      0\n      Model\n      708\n      39999\n      0.02\n      0.027933\n      1.0000\n      Satisfactory\n    \n    \n      1\n      Delinquency: 1 Account\n      3\n      54\n      0.06\n      0.069519\n      0.6307\n      Not Satisfactory\n    \n    \n      2\n      Delinquency: 2 Accounts\n      0\n      6\n      0.00\n      0.142857\n      0.8352\n      Not Satisfactory\n    \n    \n      3\n      Delinquency: None\n      351\n      19939\n      0.02\n      0.027792\n      1.0000\n      Satisfactory\n    \n    \n      4\n      Grade A\n      2\n      3341\n      0.00\n      0.002272\n      0.9904\n      Satisfactory\n    \n    \n      5\n      Grade B\n      13\n      6023\n      0.00\n      0.007203\n      1.0000\n      Satisfactory\n    \n    \n      6\n      Grade C\n      24\n      5318\n      0.00\n      0.014655\n      1.0000\n      Satisfactory\n    \n    \n      7\n      Grade D\n      84\n      3133\n      0.03\n      0.043563\n      1.0000\n      Satisfactory\n    \n    \n      8\n      Grade E\n      120\n      1509\n      0.08\n      0.107363\n      0.9999\n      Satisfactory\n    \n    \n      9\n      Grade F\n      82\n      537\n      0.15\n      0.178182\n      0.9408\n      Satisfactory\n    \n    \n      10\n      Grade G\n      29\n      139\n      0.21\n      0.243961\n      0.8338\n      Not Satisfactory\n    \n  \n\n\n\n\n\n\nSend results to ValidMind\n\n# Test passed only if all values for 'Outcome' are 'Satisfactory'\npassed = results['Outcome'].all() == 'Satisfactory'\npassed\n\nFalse\n\n\n\n# Build a vm.TestResult object for each row in the results dataframe\ntest_results = []\nfor index, row in results.iterrows():\n    test_results.append(vm.TestResult(\n        passed=row['Outcome'] == 'Satisfactory',\n        values={\n            'segment': row['Segment'],\n            'defaults': row['Defaults'],\n            'observations': row['Observations'],\n            'default_rate': row['Default Rate'],\n            'calibrated_pd': row['Calibrated PD'],\n            'p_value': row['P-value']\n        }\n    ))\n\n\njeffreys_params = {\n    \"threshold\": 0.85\n}\n\njeffreys_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"jeffreys_test\",\n    params=jeffreys_params,\n    passed=passed,\n    results=test_results,\n)\n\n\nvm.log_test_results([\n    jeffreys_test_result\n])\n\nSuccessfully logged test results for test: jeffreys_test\n\n\nTrue"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/send_custom_metrics.html",
    "href": "notebooks/send_custom_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "Send custom metrics\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\n# Send custom metrics to the API. Depending on the metric's type, scope and key, they will\n# be displayed in the ValidMind dashboard according to the template defined by the validator\n\n# TODO: document the values allowed for type, scope and key\naccuracy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"accuracy\",\n    value=[0.666]\n)\n\n# This metric won't show up in the UI because there's no component defined for it\nmy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"my_custom_metric\",\n    value=[0.1]\n)\n\n\nvm.log_metrics([accuracy_metric, my_metric])\n\nSuccessfully logged metrics\n\n\nTrue\n\n\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_score\",\n    params=custom_params,\n    passed=False,\n    results=[\n        vm.TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nSuccessfully logged test results for test: accuracy_score\n\n\nTrue"
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  },
  {
    "objectID": "guide/developerframework.html",
    "href": "guide/developerframework.html",
    "title": "Developer framework",
    "section": "",
    "text": "The ValidMind developer framework allows model developers and validators to automatically document different aspects of the model development lifecycle.\nThis Python library provides the following high level features:"
  },
  {
    "objectID": "guide/developerframework.html#installing-the-client-library",
    "href": "guide/developerframework.html#installing-the-client-library",
    "title": "Developer framework",
    "section": "Installing the client library",
    "text": "Installing the client library\nThe Python library can be installed with the following command:\npip install validmind"
  },
  {
    "objectID": "guide/developerframework.html#initializing-the-client-library",
    "href": "guide/developerframework.html#initializing-the-client-library",
    "title": "Developer framework",
    "section": "Initializing the client library",
    "text": "Initializing the client library\nEvery validation project has a project identifier that allows the client library to associate documentation and tests with the appropriate project. In order to initialize the client, we need to provide the following arguments:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nLocation of the ValidMind API.\n\n\napi_key\nAccount API key.\n\n\napi_secret\nAccount Secret key.\n\n\nproject\nThe project identifier.\n\n\n\nThe following code snippet shows how to initialize a ValidMind client instance:\nimport validmind as vm\n\nvm.init(\n  api_host = \"<API_HOST>\",\n  api_key = \"<API_KEY>\",\n  api_secret = \"<API_SECRET>\",\n  project = \"<PROJECT_ID>\"\n)"
  },
  {
    "objectID": "releasenotes.html",
    "href": "releasenotes.html",
    "title": "Release notes",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "releasenotes.html#changelog",
    "href": "releasenotes.html#changelog",
    "title": "Release notes",
    "section": "Changelog",
    "text": "Changelog\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "releasenotes.html#bug-fixes",
    "href": "releasenotes.html#bug-fixes",
    "title": "Release notes",
    "section": "Bug fixes",
    "text": "Bug fixes\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/releasenotes.html",
    "href": "guide/releasenotes.html",
    "title": "Release notes",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/releasenotes.html#changelog",
    "href": "guide/releasenotes.html#changelog",
    "title": "Release notes",
    "section": "Changelog",
    "text": "Changelog\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/releasenotes.html#bug-fixes",
    "href": "guide/releasenotes.html#bug-fixes",
    "title": "Release notes",
    "section": "Bug fixes",
    "text": "Bug fixes\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/jupyternotebooks.html",
    "href": "guide/jupyternotebooks.html",
    "title": "Jupyter notebooks",
    "section": "",
    "text": "Our Jupyter notebooks are designed to showcase the capabilities and features of the ValidMind platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nOur notebooks cover a variety of use cases, ranging from a basic introductory one fpr data analysis and visualization techniques to more advanced machine learning algorithms and data science workflows for model risk management."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support page is designed to provide you with quick and easy access to the resources you need to troubleshoot technical issues, find answers to frequently asked questions, and get the most out of our ValidMind platform.\nDon’t see what you are looking for? You can also email support to get more help."
  }
]