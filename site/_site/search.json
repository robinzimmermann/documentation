[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Get started as a developer\nGet started as a validator\nGet started as an auditor\nFind all the information you need to use our platform for model risk management (MRM).\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/lending_club.html",
    "href": "notebooks/lending_club.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/lending_club.html#validmind-sdk-introduction",
    "href": "notebooks/lending_club.html#validmind-sdk-introduction",
    "title": "ValidMind",
    "section": "ValidMind SDK Introduction",
    "text": "ValidMind SDK Introduction\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the library code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local library code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\n# Use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\" if you want to connect to the dev environment\nvm.init(\n    project=\"cl1jyv16o000809lg98gi9tie\"\n)\n\nTrue\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\nBefore we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n\n\n\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured content_id can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the Model Overview section for a project, they can use model_overview as the content_id:\nvm.log_metadata(\"model_overview\", text=\"Testing\")\nThe text argument accepts Markdown formatted text as we’ll see in the cell below. The documentation used for this model has been taken from the Kaggle dataset.\n\nmodel_overview = \"\"\"\nTesting writing metadata from the framework\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nThe dashboard should now display the Model Overview section with the text we have provided from the library:\n\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataQuality test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_data_quality) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nRunning test plan 'tabular_data_quality'...\n\n\n\n\n\nRunning ThresholdTest: class_imbalance\nRunning ThresholdTest: duplicates\nRunning ThresholdTest: cardinality\nRunning ThresholdTest: pearson_correlation\nRunning ThresholdTest: missing\nRunning ThresholdTest: skewness\nRunning ThresholdTest: unique\nRunning ThresholdTest: zeros\nSending results of test plan execution 'tabular_data_quality' to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: unique\nSuccessfully logged test results for test: zeros\n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                           Name                        Description                                   \n\n\nsklearn_classifier_metrics   SKLearnClassifierMetrics    Test plan for sklearn classifier metrics      \nsklearn_classifier_validationSKLearnClassifierPerformanceTest plan for sklearn classifier models       \nsklearn_classifier           SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                               \ntabular_dataset              TabularDataset              Test plan for generic tabular datasets        \ntabular_dataset_description  TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                               \ntabular_data_quality         TabularDataQuality          Test plan for data quality on tabular datasets\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                       Name                        Description                                                               \n\n\nMetric       dataset_correlations     DatasetCorrelations         Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                           \nMetric       dataset_description      DatasetDescription          Collects a set of descriptive statistics for a dataset                    \nCustom Test  dataset_metadata         DatasetMetadata             Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                           \nThresholdTestclass_imbalance          ClassImbalanceTest          Test that the minority class does not represent more than a threshold\n    of the total number of examples                                                                           \nThresholdTestduplicates               DuplicatesTest              Test that the number of duplicates is less than a threshold               \nThresholdTestcardinality              HighCardinalityTest         Test that the number of unique values in a column is less than a threshold\nThresholdTestpearson_correlation      HighPearsonCorrelationTest  Test that the Pearson correlation between two columns is less than a threshold\n\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           \nThresholdTestmissing                  MissingValuesTest           Test that the number of missing values is less than a threshold           \nThresholdTestskewness                 SkewnessTest                Test that the skewness of a column is less than a threshold               \nThresholdTestunique                   UniqueRowsTest              Test that the number of unique rows is greater than a threshold           \nThresholdTestzeros                    ZerosTest                   Test that the number of zeros is less than a threshold                    \nMetric       accuracy                 AccuracyScore               Accuracy Score                                                            \nMetric       csi                      CharacteristicStabilityIndexCharacteristic Stability Index between two datasets                       \nMetric       confusion_matrix         ConfusionMatrix             Confusion Matrix                                                          \nMetric       f1_score                 F1Score                     F1 Score                                                                  \nMetric       pfi                      PermutationFeatureImportancePermutation Feature Importance                                            \nMetric       psi                      PopulationStabilityIndex    Population Stability Index between two datasets                           \nMetric       pr_curve                 PrecisionRecallCurve        Precision Recall Curve                                                    \nMetric       precision                PrecisionScore              Precision Score                                                           \nMetric       roc_auc                  ROCAUCScore                 ROC AUC Score                                                             \nMetric       roc_curve                ROCCurve                    ROC Curve                                                                 \nMetric       recall                   RecallScore                 Recall Score                                                              \nCustom Test  shap                     SHAPGlobalImportance        SHAP Global Importance. Custom metric                                     \nThresholdTestaccuracy_score           AccuracyTest                Test that the accuracy score is above a threshold.                        \nThresholdTestf1_score                 F1ScoreTest                 Test that the F1 score is above a threshold.                              \nThresholdTestroc_auc_score            ROCAUCScoreTest             Test that the ROC AUC score is above a threshold.                         \nThresholdTesttraining_test_degradationTrainingTestDegradationTest Test that the training set metrics are better than the test set metrics.  \n\n\n\n\nOnce the TabularDataset test plan has finished running, we can view the results in the ValidMind dashboard:\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.865625\n\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning test plan 'sklearn_classifier'...\nGenerating predictions train dataset...\nGenerating predictions test dataset...\n\n\n\n\n\nSending results of test plan execution 'sklearn_classifier' to ValidMind...\n|-- Running sub test plan - sklearn_classifier_metrics\nRunning test plan 'sklearn_classifier_metrics'...\n\n\n\n\n\nRunning ModelMetadata: model_metadata\nRunning Metric: accuracy\nRunning Metric: confusion_matrix\nRunning Metric: f1_score\nRunning Metric: pfi\nRunning Metric: pr_curve\nRunning Metric: precision\nRunning Metric: recall\nRunning Metric: roc_auc\nRunning Metric: roc_curve\nRunning Metric: csi\nRunning Metric: psi\nRunning SHAPGlobalImportance: shap\n\n\nntree_limit is deprecated, use `iteration_range` or model slicing instead.\n\n\nSending results of test plan execution 'sklearn_classifier_metrics' to ValidMind...\nSuccessfully logged metrics\n|-- Running sub test plan - sklearn_classifier_validation\nRunning test plan 'sklearn_classifier_validation'...\n\n\n\n\n\nRunning ThresholdTest: accuracy_score\nRunning ThresholdTest: f1_score\nRunning ThresholdTest: roc_auc_score\nRunning ThresholdTest: training_test_degradation\nSending results of test plan execution 'sklearn_classifier_validation' to ValidMind...\nSuccessfully logged test results for test: accuracy_score\nSuccessfully logged test results for test: f1_score\nSuccessfully logged test results for test: roc_auc_score\nSuccessfully logged test results for test: training_test_degradation"
  },
  {
    "objectID": "notebooks/intro.html#validmind-python-library-introduction",
    "href": "notebooks/intro.html#validmind-python-library-introduction",
    "title": "ValidMind",
    "section": "ValidMind Python Library Introduction",
    "text": "ValidMind Python Library Introduction\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/intro.html#initializing-the-validmind-library",
    "href": "notebooks/intro.html#initializing-the-validmind-library",
    "title": "ValidMind",
    "section": "Initializing the ValidMind Library",
    "text": "Initializing the ValidMind Library\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\n# Use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\" if you want to connect to the dev environment\nvm.init(\n    project=\"cl1jyv16o000809lg98gi9tie\"\n)\n\nTrue\n\n\n\nUsing a demo dataset\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\nBefore we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n\n\nLogging general project metadata with log_metadata\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured content_id can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the Model Overview section for a project, they can use model_overview as the content_id:\nvm.log_metadata(\"model_overview\", text=\"Testing\")\nThe text argument accepts Markdown formatted text as we’ll see in the cell below. The documentation used for this model has been taken from the Kaggle dataset.\n\nmodel_overview = \"\"\"\nTesting writing metadata from the framework\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nThe dashboard should now display the Model Overview section with the text we have provided from the library:\n\n\n\n\nRunning a data quality test plan\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\nLoad our demo dataset\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nInitialize and run the TabularDataQuality test plan\nWe can now initialize the TabularDataQuality test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_data_quality) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nvm.run_test_plan(\"tabular_data_quality\", dataset=vm_dataset)\n\nRunning test plan 'tabular_data_quality'...\n\n\n\n\n\nRunning ThresholdTest: class_imbalance\nRunning ThresholdTest: duplicates\nRunning ThresholdTest: cardinality\nRunning ThresholdTest: pearson_correlation\nRunning ThresholdTest: missing\nRunning ThresholdTest: skewness\nRunning ThresholdTest: unique\nRunning ThresholdTest: zeros\nSending results of test plan execution 'tabular_data_quality' to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: unique\nSuccessfully logged test results for test: zeros\n\n\n\n\n\nFinding all test plans available in the developer framework\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                           Name                        Description                                   \n\n\nsklearn_classifier_metrics   SKLearnClassifierMetrics    Test plan for sklearn classifier metrics      \nsklearn_classifier_validationSKLearnClassifierPerformanceTest plan for sklearn classifier models       \nsklearn_classifier           SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                               \ntabular_dataset              TabularDataset              Test plan for generic tabular datasets        \ntabular_dataset_description  TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                               \ntabular_data_quality         TabularDataQuality          Test plan for data quality on tabular datasets\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                       Name                        Description                                                               \n\n\nMetric       dataset_correlations     DatasetCorrelations         Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                           \nMetric       dataset_description      DatasetDescription          Collects a set of descriptive statistics for a dataset                    \nCustom Test  dataset_metadata         DatasetMetadata             Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                           \nThresholdTestclass_imbalance          ClassImbalanceTest          Test that the minority class does not represent more than a threshold\n    of the total number of examples                                                                           \nThresholdTestduplicates               DuplicatesTest              Test that the number of duplicates is less than a threshold               \nThresholdTestcardinality              HighCardinalityTest         Test that the number of unique values in a column is less than a threshold\nThresholdTestpearson_correlation      HighPearsonCorrelationTest  Test that the Pearson correlation between two columns is less than a threshold\n\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           \nThresholdTestmissing                  MissingValuesTest           Test that the number of missing values is less than a threshold           \nThresholdTestskewness                 SkewnessTest                Test that the skewness of a column is less than a threshold               \nThresholdTestunique                   UniqueRowsTest              Test that the number of unique rows is greater than a threshold           \nThresholdTestzeros                    ZerosTest                   Test that the number of zeros is less than a threshold                    \nMetric       accuracy                 AccuracyScore               Accuracy Score                                                            \nMetric       csi                      CharacteristicStabilityIndexCharacteristic Stability Index between two datasets                       \nMetric       confusion_matrix         ConfusionMatrix             Confusion Matrix                                                          \nMetric       f1_score                 F1Score                     F1 Score                                                                  \nMetric       pfi                      PermutationFeatureImportancePermutation Feature Importance                                            \nMetric       psi                      PopulationStabilityIndex    Population Stability Index between two datasets                           \nMetric       pr_curve                 PrecisionRecallCurve        Precision Recall Curve                                                    \nMetric       precision                PrecisionScore              Precision Score                                                           \nMetric       roc_auc                  ROCAUCScore                 ROC AUC Score                                                             \nMetric       roc_curve                ROCCurve                    ROC Curve                                                                 \nMetric       recall                   RecallScore                 Recall Score                                                              \nCustom Test  shap                     SHAPGlobalImportance        SHAP Global Importance. Custom metric                                     \nThresholdTestaccuracy_score           AccuracyTest                Test that the accuracy score is above a threshold.                        \nThresholdTestf1_score                 F1ScoreTest                 Test that the F1 score is above a threshold.                              \nThresholdTestroc_auc_score            ROCAUCScoreTest             Test that the ROC AUC score is above a threshold.                         \nThresholdTesttraining_test_degradationTrainingTestDegradationTest Test that the training set metrics are better than the test set metrics.  \n\n\n\n\nOnce the TabularDataset test plan has finished running, we can view the results in the ValidMind dashboard:\n\n\n\nPreparing the dataset for training\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\nDropping irrelevant variables\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\nEncoding categorical variables\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\nDataset preparation\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\nModel training\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.865625\n\n\n\n\nRunning a model evaluation test plan\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\nInitialize VM model object and train/test datasets\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning test plan 'sklearn_classifier'...\nGenerating predictions train dataset...\nGenerating predictions test dataset...\n\n\n\n\n\nSending results of test plan execution 'sklearn_classifier' to ValidMind...\n|-- Running sub test plan - sklearn_classifier_metrics\nRunning test plan 'sklearn_classifier_metrics'...\n\n\n\n\n\nRunning ModelMetadata: model_metadata\nRunning Metric: accuracy\nRunning Metric: confusion_matrix\nRunning Metric: f1_score\nRunning Metric: pfi\nRunning Metric: pr_curve\nRunning Metric: precision\nRunning Metric: recall\nRunning Metric: roc_auc\nRunning Metric: roc_curve\nRunning Metric: csi\nRunning Metric: psi\nRunning SHAPGlobalImportance: shap\n\n\nntree_limit is deprecated, use `iteration_range` or model slicing instead.\n\n\nSending results of test plan execution 'sklearn_classifier_metrics' to ValidMind...\nSuccessfully logged metrics\n|-- Running sub test plan - sklearn_classifier_validation\nRunning test plan 'sklearn_classifier_validation'...\n\n\n\n\n\nRunning ThresholdTest: accuracy_score\nRunning ThresholdTest: f1_score\nRunning ThresholdTest: roc_auc_score\nRunning ThresholdTest: training_test_degradation\nSending results of test plan execution 'sklearn_classifier_validation' to ValidMind...\nSuccessfully logged test results for test: accuracy_score\nSuccessfully logged test results for test: f1_score\nSuccessfully logged test results for test: roc_auc_score\nSuccessfully logged test results for test: training_test_degradation"
  },
  {
    "objectID": "notebooks/library_intro_demos.html",
    "href": "notebooks/library_intro_demos.html",
    "title": "ValidMind",
    "section": "",
    "text": "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle.\nFor modelers, the client provides the following high level features:\n\nLog qualitative data about the model’s conceptual soundness\nLog information about datasets and models\nLog training and evaluation metrics about datasets and models\nRun data quality checks\nRun model evaluation tests\n\nFor validators, the client also provides (TBD) the ability to effectively challenge the model’s performance according to its objective, use case and specific project’s requirements.\n\n\n\nThis notebook and the ValidMind client must be executed on an environment running Python >= 3.8.\n\n\n\n\nWhile we finish the process of making the library publicly accessible pip, it can be installed with the following command that will direct pip to the S3 bucket that contains the latest version of the client.\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv('./env')\n\nTrue\n\n\n\n\n\n\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\n\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_metadata\nLogs free-form metadata text for a given content ID in the model documentation\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\n\nLogs free-form metadata text for a given content ID in the model documentation.\nArguments:\n\ncontent_id: Content ID of the model documentation. This is a unique identifier generated by the ValidMind dashboard. See available content_ids in the model training section below\ntext: Free-form text to be logged. A text template can be specified in combination with extra_json (see below)\nextra_json: (TBD support for this) JSON object containing variables to be substituted in the text template\n\n\n\n\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\n\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)\n\n\n\n\n\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\n\n\nBefore we start logging information about our dataset, we’d want to send metadata to ValidMind about the model’s conceptual soundness, for example. Model developers have the option to directly populate parts of the dashboard documentation using special content_ids. The following is the list of content_ids supported at the moment:\n\n\n\n\n\n\n\nContent ID\nPopulates Section\n\n\n\n\nmodel_overview\nConceptual Soundness -> Model Overview\n\n\nmodel_selection\nConceptual Soundness -> Model Selection\n\n\nbusiness_case\nConceptual Soundness -> Intended Use and Business Use Case\n\n\nfeature_selection\nData Preparation -> Feature Selection and Engineering\n\n\ngovernance_plan\nMonitoring and Governance -> Governance Plan\n\n\nmonitoring_implementation\nMonitoring and Governance -> Monitoring Implementation\n\n\nmonitoring_plan\nMonitoring and Governance -> Monitoring Plan\n\n\n\nIn the following log_metadata example, we will populate the Model Overview section in the dashboard:\n\nmodel_overview = \"\"\"\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\n\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\n\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\n\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/library_intro_demos.html#creating-a-new-project",
    "href": "notebooks/library_intro_demos.html#creating-a-new-project",
    "title": "ValidMind",
    "section": "Creating a new project",
    "text": "Creating a new project\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\nInitializing the client library\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/library_intro_demos.html#validmind-client-library-functions",
    "href": "notebooks/library_intro_demos.html#validmind-client-library-functions",
    "title": "ValidMind",
    "section": "ValidMind Client Library Functions",
    "text": "ValidMind Client Library Functions\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_metadata\nLogs free-form metadata text for a given content ID in the model documentation\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\nlog_metadata\nLogs free-form metadata text for a given content ID in the model documentation.\nArguments:\n\ncontent_id: Content ID of the model documentation. This is a unique identifier generated by the ValidMind dashboard. See available content_ids in the model training section below\ntext: Free-form text to be logged. A text template can be specified in combination with extra_json (see below)\nextra_json: (TBD support for this) JSON object containing variables to be substituted in the text template\n\n\n\nanalyze_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\nevaluate_model\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)"
  },
  {
    "objectID": "notebooks/library_intro_demos.html#training-an-example-model",
    "href": "notebooks/library_intro_demos.html#training-an-example-model",
    "title": "ValidMind",
    "section": "Training an Example Model",
    "text": "Training an Example Model\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\n\nlog_metadata\nBefore we start logging information about our dataset, we’d want to send metadata to ValidMind about the model’s conceptual soundness, for example. Model developers have the option to directly populate parts of the dashboard documentation using special content_ids. The following is the list of content_ids supported at the moment:\n\n\n\n\n\n\n\nContent ID\nPopulates Section\n\n\n\n\nmodel_overview\nConceptual Soundness -> Model Overview\n\n\nmodel_selection\nConceptual Soundness -> Model Selection\n\n\nbusiness_case\nConceptual Soundness -> Intended Use and Business Use Case\n\n\nfeature_selection\nData Preparation -> Feature Selection and Engineering\n\n\ngovernance_plan\nMonitoring and Governance -> Governance Plan\n\n\nmonitoring_implementation\nMonitoring and Governance -> Monitoring Implementation\n\n\nmonitoring_plan\nMonitoring and Governance -> Monitoring Plan\n\n\n\nIn the following log_metadata example, we will populate the Model Overview section in the dashboard:\n\nmodel_overview = \"\"\"\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\nanalyze_dataset\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\nPreparing the training dataset\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\nevaluate_model\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/lending_club_regression.html",
    "href": "notebooks/lending_club_regression.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/lending_club_regression.html#validmind-sdk-introduction",
    "href": "notebooks/lending_club_regression.html#validmind-sdk-introduction",
    "title": "ValidMind",
    "section": "ValidMind SDK Introduction",
    "text": "ValidMind SDK Introduction\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/explore_x_train_lc.html",
    "href": "notebooks/explore_x_train_lc.html",
    "title": "ValidMind",
    "section": "",
    "text": "Explore x to train LC\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# PD Model\nvm.init(project=\"cl1jyvh2c000909lg1rk0a0zb\")\n\nTrue\n\n\n\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\n\ndef jeffreys_test(p: float, n: int = 0, d: int = 0) -> float:\n    \"\"\"\n    Perform a test that the test probability, p, is consistent with the observed number of \n    successes, d, from a number of trials, n.\n\n    This uses the Jeffrey's posterior probability, which is the Beta distribution with shape\n    parameters a = d + 1/2 and b = n - d + 1/2. The result is the one sided p-value representing the \n    probability that the test probability, p, is greater than the true probability.\n\n    :param p: the test probability to be compared to the number of successes given n trials\n    :param n: the number of trials\n    :param d: the number of successes [optional, default = 0]\n\n    :return p-value: one sided p-value of the test\n    \"\"\"\n    return scipy.stats.beta.cdf(p, d + 0.5, n - d + 0.5)\n\n\ndef update_result(s, d, n, dr, p, pval, out = 'Yet to decide'):\n    return ({'Segment': s,\n            'Defaults': d,\n            'Observations': n,\n            'Default Rate': dr,\n            'Calibrated PD': p,\n            'P-value': pval, \n            'Outcome': out})\n\n\ndef calculate_and_return(df = pd.DataFrame, cal_pd = {}, pool = None, obs = 'observed', threshold = 0.9):\n    \"\"\"\n    Take the input dataframe, analyse & clean, seprate poolwise.\n    Calculate the jeffreys statistic\n    \"\"\"\n    \n    result = pd.DataFrame(columns = ['Segment', 'Defaults', 'Observations', 'Default Rate', 'Calibrated PD', 'P-value', 'Outcome'])\n    \n    n = len(df[obs])\n    d = np.sum(df[obs])\n    dr = np.round(d/n,2)\n    p = cal_pd['Model']\n    pval = np.round(jeffreys_test(p, n, d),4)\n    if pval>=threshold:\n        out = 'Satisfactory'\n    else:\n        out = 'Not Satisfactory'\n    \n    result = result.append(update_result('Model', d, n, dr, p, pval, out), ignore_index = True)\n    \n    if pool != None:\n        samples = df.groupby(pool)\n        \n        for sample in samples:\n            n = len(sample[1][obs])\n            d = np.sum(sample[1][obs])\n            dr = np.round(d/n,2)\n            p = cal_pd[sample[0]]\n            pval = np.round(jeffreys_test(p, n, d),4)\n            \n            if pval>=threshold:\n                out = 'Satisfactory'\n            else:\n                out = 'Not Satisfactory'\n            \n            result = result.append(update_result(sample[0], d, n, dr, p, pval, out), ignore_index = True)\n            \n    return result\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/x_train_lc.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      28000.0\n      7.12\n      10\n      125000.0\n      15.97\n      0.0\n      26\n      725.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      11200.0\n      10.99\n      2\n      80600.0\n      15.93\n      0.0\n      15\n      670.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      2\n      14000.0\n      15.10\n      6\n      83000.0\n      18.17\n      0.0\n      13\n      660.0\n      1.0\n      76.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      12725.0\n      12.12\n      6\n      71300.0\n      29.70\n      0.0\n      13\n      675.0\n      2.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      7200.0\n      15.31\n      1\n      25000.0\n      32.98\n      0.0\n      8\n      700.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ndf[\"acc_now_delinq\"].value_counts()\n\n0.0    59802\n1.0      187\n2.0        7\n3.0        3\n5.0        1\nName: acc_now_delinq, dtype: int64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 60.0 MB\n\n\n\ntest_df = pd.read_csv(\"./notebooks/datasets/_temp/x_test_lc.csv\")\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      15500.0\n      8.90\n      10\n      100000.0\n      0.74\n      0.0\n      14\n      715.0\n      3.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      10800.0\n      11.67\n      10\n      68000.0\n      15.44\n      1.0\n      20\n      670.0\n      1.0\n      8.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      2\n      15850.0\n      15.10\n      2\n      36000.0\n      26.50\n      0.0\n      31\n      720.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      16000.0\n      15.31\n      2\n      80000.0\n      24.54\n      1.0\n      12\n      705.0\n      0.0\n      21.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      4\n      14000.0\n      12.12\n      10\n      90000.0\n      14.07\n      0.0\n      14\n      695.0\n      1.0\n      44.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ntest_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 20.0 MB\n\n\n\nmodel = joblib.load(\"./notebooks/datasets/_temp/lc_model.pickle\")\n\n\nsegments = [\n    {\n        \"name\": \"Grade\",\n        \"segments\": [\n            {\"name\": \"Grade A\", \"query\": \"grade_A == 1\"},\n            {\"name\": \"Grade B\", \"query\": \"grade_B == 1\"},\n            {\"name\": \"Grade C\", \"query\": \"grade_C == 1\"},\n            {\"name\": \"Grade D\", \"query\": \"grade_D == 1\"},\n            {\"name\": \"Grade E\", \"query\": \"grade_E == 1\"},\n            {\"name\": \"Grade F\", \"query\": \"grade_F == 1\"},\n            {\"name\": \"Grade G\", \"query\": \"grade_G == 1\"},\n        ]\n    },\n    {\n        \"name\": \"Delinquency\",\n        \"segments\": [\n            {\"name\": \"Delinquency: None\", \"query\": \"acc_now_delinq == 0\"},\n            {\"name\": \"Delinquency: 1 Account\", \"query\": \"acc_now_delinq == 1\"},\n            {\"name\": \"Delinquency: 2 Accounts\", \"query\": \"acc_now_delinq == 2\"},\n        ]\n    }\n]\n\n\ndef get_calibrated_pds(df, model, segments):\n    model_preds = model.predict_proba(df)[:, 1]\n    model_class_preds = (model_preds > 0.5).astype(int)\n\n    pds = {\"Model\": model_class_preds.sum() / len(model_class_preds)}\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            total_pds = class_pred.sum()\n            segment_pd = total_pds / len(class_pred)\n\n            pds[segment[\"name\"]] = segment_pd\n    return pds\n\n\ncalibrated_pds = get_calibrated_pds(df, model, segments)\ncalibrated_pds\n\n{'Model': 0.027933333333333334,\n 'Grade A': 0.0022715539494062983,\n 'Grade B': 0.007202947160059383,\n 'Grade C': 0.014655226404459197,\n 'Grade D': 0.043563336766220394,\n 'Grade E': 0.10736266241167085,\n 'Grade F': 0.1781818181818182,\n 'Grade G': 0.24396135265700483,\n 'Delinquency: None': 0.027791712651750778,\n 'Delinquency: 1 Account': 0.06951871657754011,\n 'Delinquency: 2 Accounts': 0.14285714285714285}\n\n\n\ndef process_observations(df, model, segments):\n    test_input = pd.DataFrame(columns = ['Segment', 'Observed'])\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            # Concat to test_input by adding all rows of class_pred and segment as a single value\n            test_input = pd.concat([test_input, pd.DataFrame({'Segment': segment[\"name\"], 'Observed': class_pred})], ignore_index=True)\n\n    return test_input\n\n\nobservations = process_observations(test_df, model, segments)\n\n\nresults = calculate_and_return(\n    observations,\n    cal_pd=calibrated_pds,\n    pool = 'Segment',\n    obs=\n    'Observed',\n    threshold = 0.85\n)\nresults\n\n\n\n\n\n  \n    \n      \n      Segment\n      Defaults\n      Observations\n      Default Rate\n      Calibrated PD\n      P-value\n      Outcome\n    \n  \n  \n    \n      0\n      Model\n      708\n      39999\n      0.02\n      0.027933\n      1.0000\n      Satisfactory\n    \n    \n      1\n      Delinquency: 1 Account\n      3\n      54\n      0.06\n      0.069519\n      0.6307\n      Not Satisfactory\n    \n    \n      2\n      Delinquency: 2 Accounts\n      0\n      6\n      0.00\n      0.142857\n      0.8352\n      Not Satisfactory\n    \n    \n      3\n      Delinquency: None\n      351\n      19939\n      0.02\n      0.027792\n      1.0000\n      Satisfactory\n    \n    \n      4\n      Grade A\n      2\n      3341\n      0.00\n      0.002272\n      0.9904\n      Satisfactory\n    \n    \n      5\n      Grade B\n      13\n      6023\n      0.00\n      0.007203\n      1.0000\n      Satisfactory\n    \n    \n      6\n      Grade C\n      24\n      5318\n      0.00\n      0.014655\n      1.0000\n      Satisfactory\n    \n    \n      7\n      Grade D\n      84\n      3133\n      0.03\n      0.043563\n      1.0000\n      Satisfactory\n    \n    \n      8\n      Grade E\n      120\n      1509\n      0.08\n      0.107363\n      0.9999\n      Satisfactory\n    \n    \n      9\n      Grade F\n      82\n      537\n      0.15\n      0.178182\n      0.9408\n      Satisfactory\n    \n    \n      10\n      Grade G\n      29\n      139\n      0.21\n      0.243961\n      0.8338\n      Not Satisfactory\n    \n  \n\n\n\n\n\n\nSend results to ValidMind\n\n# Test passed only if all values for 'Outcome' are 'Satisfactory'\npassed = results['Outcome'].all() == 'Satisfactory'\npassed\n\nFalse\n\n\n\n# Build a vm.TestResult object for each row in the results dataframe\ntest_results = []\nfor index, row in results.iterrows():\n    test_results.append(vm.TestResult(\n        passed=row['Outcome'] == 'Satisfactory',\n        values={\n            'segment': row['Segment'],\n            'defaults': row['Defaults'],\n            'observations': row['Observations'],\n            'default_rate': row['Default Rate'],\n            'calibrated_pd': row['Calibrated PD'],\n            'p_value': row['P-value']\n        }\n    ))\n\n\njeffreys_params = {\n    \"threshold\": 0.85\n}\n\njeffreys_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"jeffreys_test\",\n    params=jeffreys_params,\n    passed=passed,\n    results=test_results,\n)\n\n\nvm.log_test_results([\n    jeffreys_test_result\n])\n\nSuccessfully logged test results for test: jeffreys_test\n\n\nTrue"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/send_custom_metrics.html",
    "href": "notebooks/send_custom_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "Send custom metrics\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\n# Send custom metrics to the API. Depending on the metric's type, scope and key, they will\n# be displayed in the ValidMind dashboard according to the template defined by the validator\n\n# TODO: document the values allowed for type, scope and key\naccuracy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"accuracy\",\n    value=[0.666]\n)\n\n# This metric won't show up in the UI because there's no component defined for it\nmy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"my_custom_metric\",\n    value=[0.1]\n)\n\n\nvm.log_metrics([accuracy_metric, my_metric])\n\nSuccessfully logged metrics\n\n\nTrue\n\n\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_score\",\n    params=custom_params,\n    passed=False,\n    results=[\n        vm.TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nSuccessfully logged test results for test: accuracy_score\n\n\nTrue"
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  },
  {
    "objectID": "notebooks/log_image.html#validmind-sdk-introduction",
    "href": "notebooks/log_image.html#validmind-sdk-introduction",
    "title": "ValidMind",
    "section": "ValidMind SDK Introduction",
    "text": "ValidMind SDK Introduction\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0"
  },
  {
    "objectID": "notebooks/log_image.html#log-image-from-path",
    "href": "notebooks/log_image.html#log-image-from-path",
    "title": "ValidMind",
    "section": "Log Image from path",
    "text": "Log Image from path\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}"
  },
  {
    "objectID": "notebooks/log_image.html#log-plot",
    "href": "notebooks/log_image.html#log-plot",
    "title": "ValidMind",
    "section": "Log plot",
    "text": "Log plot\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}"
  },
  {
    "objectID": "notebooks/log_image.html#log-plot-with-seaborn",
    "href": "notebooks/log_image.html#log-plot-with-seaborn",
    "title": "ValidMind",
    "section": "Log plot with seaborn",
    "text": "Log plot with seaborn\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Get started",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "getstarteddeveloper.html",
    "href": "getstarteddeveloper.html",
    "title": "Get started as a developer",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "getstartedvalidator.html",
    "href": "getstartedvalidator.html",
    "title": "Get started as a validator",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "getstartedauditor.html",
    "href": "getstartedauditor.html",
    "title": "Get started as an auditor",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "python-library/index.html",
    "href": "python-library/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "Skip to content"
  },
  {
    "objectID": "python-library/index.html#commands",
    "href": "python-library/index.html#commands",
    "title": "ValidMind",
    "section": "Commands",
    "text": "Commands\n\nmkdocs new [dir-name] - Create a new project.\nmkdocs serve - Start the live-reloading docs server.\nmkdocs build - Build the documentation site.\nmkdocs -h - Print help message and exit."
  },
  {
    "objectID": "python-library/index.html#project-layout",
    "href": "python-library/index.html#project-layout",
    "title": "ValidMind",
    "section": "Project layout",
    "text": "Project layout\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n\n\n\nExports\n\n\nDataset dataclass\n\nModel class wrapper\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n\n\n@dataclass()\nclass Dataset:\n    \"\"\"\n    Model class wrapper\n    \"\"\"\n\n    raw_dataset: object\n    fields: list\n    variables: list\n    sample: list\n    shape: dict\ncorrelation_matrix: object = None correlations: dict = None type: str = None options: dict = None statistics: dict = None\n    # Specif\ny targets via DatasetTargets or v ia target_column and class_labels targets: dict = None target_column: str = “” class_labels: dict = None\n    __feature_lookup: d\nict = field(default_factory=dict)\n__transformed_df: object = None\n    def __post_init__(self):\n        \"\"\"\n        Set target_column and\nclass_labels from DatasetTargets ““” if self.targets: self.target_co lumn = self.targets.target_column self.class_l abels = self.targets.class_labels\n    @property\n    def x(self):\n        \"\"\"\nReturns the dataset’s features ““”\n      return self.raw_dataset\n.drop(self.target_column, axis=1)\n    @property\n    def y(self):\n        \"\"\"\n        Re\nturns the dataset’s target column ““” return sel f.raw_dataset[self.target_column]\n    def get\n_feature_by_id(self, feature_id): ““”\n   Returns the feature with t\nhe given id. We also build a lazy\nlookup cache in case the same fea ture is requested multiple times. ““” if feature _id not in self.__feature_lookup:\n  for feature in self.fields:\nif feature[“id”] == feature_id:\n                   self.__fea\nture_lookup[feature_id] = feature\n               return feature\nraise ValueError(f”Feature with id {feature_id} does not exist”)\n        return\nself.__feature_lookup[feature_id]\n    def ge\nt_feature_type(self, feature_id): ““” Returns the type of the feature with the given id ““” feature = s elf.get_feature_by_id(feature_id)\n       return feature[\"type\"]\n\n    def serialize(self):\n        \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” dataset_dict = {\n         \"shape\": self.shape,\n\n           \"type\": self.type,\n        }\n\n        # Data\nset with no targets can be logged if self.targets: dataset_dict[” targets”] = self.targets.__dict__ else:\n  dataset_dict[\"targets\"] = {\n                \"ta\nrget_column”: self.target_column, ” class_labels”: self.class_labels, }\n        return dataset_dict\n\n    def describe(self):\n        \"\"\"\n\n Extracts descriptive statist\nics for each field in the dataset ““” transfor med_df = self.transformed_dataset\n for ds_field in self.fields:\n            describe_datase\nt_field(transformed_df, ds_field)\n  def get_correlations(self):\n        \"\"\"\n        Extracts correlati\nons for each field in the dataset ““” # Ignore field s that have very high cardinality\n  fields_for_correlation = []\n\n for ds_field in self.fields:\n            if\n“statistics” in ds_field and “dis tinct” in ds_field[“statistics”]: if ds_field [“statistics”][“distinct”] < 0.1:\n                 fields_for_c\norrelation.append(ds_field[“id”])\n        self.c\norrelation_matrix = associations( self.transformed _dataset[fields_for_correlation],\n           compute_only=True,\n            plot=False,\n        )[\"corr\"]\n\n        # Transform to the\ncurrent format expected by the UI self.correlations = [ [ {\n                \"field\": key,\n\n              \"value\": value,\n                }\n                for key,\nvalue in correlation_row.items() ] for co rrelation_row in self.correlation _matrix.to_dict(orient=“records”) ]\n    def get_c\norrelation_plots(self, n_top=15): ““” Ex tracts correlation plots for the n_top correlations in the dataset ““”\n   correlation_plots = genera\nte_correlation_plots(self, n_top)\n     return correlation_plots\n\n    @property\n    def transformed_da\ntaset(self, force_refresh=False): ““” Ret urns a transformed dataset that u ses the features from vm_dataset.\nSome of the features in vm_datase t are of type Dummy so we need to\nreverse the one hot encoding and drop the individual dummy columns ““”\nif self.__transformed_df is not None and force_refresh is False:\n return self.__transformed_df\n\n        # Get the list o\nf features that are of type Dummy\ndataset_options = self.options dummy_variables = ( dat aset_options.get(“dummy_variables”, []) if dataset_options else [] ) # Exc lude columns that have prefixes t hat are in the dummy feature list\n       dummy_column_names = [\n            column_name\n            for column\n_name in self.raw_dataset.columns if any( colum n_name.startswith(dummy_variable) for dummy_variable in dummy_variables ) ]\ntransformed_df = self.raw_dataset .drop(dummy_column_names, axis=1)\n      # Add reversed dummy fe\natures to the transformed dataset for d ummy_variable in dummy_variables:\ncolumns_with_dummy_prefix = [\n                col\n                fo\nr col in self.raw_dataset.columns\nif col.startswith(dummy_variable) ] t ransformed_df[dummy_variable] = ( self.raw_d ataset[columns_with_dummy_prefix]\n              .idxmax(axis=1)\n\n            .replace(f\"{dummy\n_variable}[-_:]“,”“, regex=True) )\n        return transformed_df\n\n    @classmethod\ndef create_from_dict(cls, dict_): class_field s = {f.name for f in fields(cls)} retur n Dataset(**{k: v for k, v in dic t_.items() if k in class_fields})\n    # TODO: Accept var\niable descriptions from framework # TODO: Acc ept type overrides from framework @classmethod def init_from_pd_dataset( cls, df, options=None, targets=None, targ et_column=None, class_labels=None ): pr int(“Inferring dataset types…”)\n   vm_dataset_variables = par\nse_dataset_variables(df, options)\n        shape = {\n\n         \"rows\": df.shape[0],\n\n      \"columns\": df.shape[1],\n        }\n        df_head = df\n.head().to_dict(orient=“records”) df_tail = df .tail().to_dict(orient=“records”)\n        # TODO: validate wi\nth target_column and class_labels if targets: validat e_pd_dataset_targets(df, targets)\n        return Dataset(\n            raw_dataset=df,\n            fields=vm_d\nataset_variables, # TODO - depre cate naming in favor of variables\nvariables=vm_dataset_variables, sample=[ {\n                \"id\": \"head\",\n\n             \"data\": df_head,\n                },\n                {\n\n                \"id\": \"tail\",\n\n             \"data\": df_tail,\n                },\n            ],\n            shape=shape,\n            targets=targets,\n\n target_column=target_column,\n\n   class_labels=class_labels,\n            options=options,\n        )\n\n\n\n\n\n\n\ntransformed_dataset property\n\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\n\n\nx property\n\nReturns the dataset's features\n\n\n\ny property\n\nReturns the dataset's target column\n\n\n\n__post_init__()\n\nSet target_column and class_labels from DatasetTargets\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n52\n53\n54\n55\n56\n57\n58\n\n\ndef __post_init__(self):\n    \"\"\"\n    Set target_column and\nclass_labels from DatasetTargets ““” if self.targets: self.target_co lumn = self.targets.target_column self.class_l abels = self.targets.class_labels\n\n\n\n\n\n\n\n\ndescribe()\n\nExtracts descriptive statistics for each field in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n115\n116\n117\n118\n119\n120\n121\n122\n\n\ndef describe(self):\n    \"\"\"\n\n Extracts descriptive statist\nics for each field in the dataset ““” transfor med_df = self.transformed_dataset\n for ds_field in self.fields:\n        describe_datase\nt_field(transformed_df, ds_field)\n\n\n\n\n\n\n\n\nget_correlation_plots(n_top=15)\n\nExtracts correlation plots for the n_top correlations in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n153\n154\n155\n156\n157\n158\n\n\ndef get_c\norrelation_plots(self, n_top=15): ““” Ex tracts correlation plots for the n_top correlations in the dataset ““”\n   correlation_plots = genera\nte_correlation_plots(self, n_top) return correlation_plots\n\n\n\n\n\n\n\n\nget_correlations()\n\nExtracts correlations for each field in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n\n\ndef get_correlations(self):\n    \"\"\"\n    Extracts correlati\nons for each field in the dataset ““” # Ignore field s that have very high cardinality\n  fields_for_correlation = []\n\n for ds_field in self.fields:\n        if\n“statistics” in ds_field and “dis tinct” in ds_field[“statistics”]: if ds_field [“statistics”][“distinct”] < 0.1: fields_for_c orrelation.append(ds_field[“id”])\n    self.c\norrelation_matrix = associations( self.transformed _dataset[fields_for_correlation], compute_only=True, plot=False, )[“corr”]\n    # Transform to the\ncurrent format expected by the UI self.correlations = [ [ { “field”: key,\n              \"value\": value,\n            }\n            for key,\nvalue in correlation_row.items() ] for co rrelation_row in self.correlation _matrix.to_dict(orient=“records”) ]\n\n\n\n\n\n\n\n\nget_feature_by_id(feature_id)\n\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\ndef get\n_feature_by_id(self, feature_id): ““”\n   Returns the feature with t\nhe given id. We also build a lazy\nlookup cache in case the same fea ture is requested multiple times. ““” if feature _id not in self.__feature_lookup:\n  for feature in self.fields:\nif feature[“id”] == feature_id: self.__fea ture_lookup[feature_id] = feature\n               return feature\nraise ValueError(f”Feature with id {feature_id} does not exist”)\n    return\nself.__feature_lookup[feature_id]\n\n\n\n\n\n\n\n\nget_feature_type(feature_id)\n\nReturns the type of the feature with the given id\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n88\n89\n90\n91\n92\n93\n\n\ndef ge\nt_feature_type(self, feature_id): ““” Returns the type of the feature with the given id ““” feature = s elf.get_feature_by_id(feature_id) return feature[“type”]\n\n\n\n\n\n\n\n\nserialize()\n\nSerializes the model to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” dataset_dict = { “shape”: self.shape, “type”: self.type, }\n    # Data\nset with no targets can be logged if self.targets: dataset_dict[” targets”] = self.targets.__dict__ else:\n  dataset_dict[\"targets\"] = {\n            \"ta\nrget_column”: self.target_column, ” class_labels”: self.class_labels, }\n    return dataset_dict\n\n\n\n\n\n\n\n\n\n\n\nDatasetTargets dataclass\n\nDataset targets definition\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n@dataclass()\nclass DatasetTargets:\n    \"\"\"\n\n   Dataset targets definition\n    \"\"\"\n\n    target_column: str\n    description: str = None\n    class_labels: dict = None\n\n\n\n\n\n\n\n\n\n\n\nFigure dataclass\n\nFigure objects track the schema supported by the ValidMind API\nSource code in validmind/vm_models/figure.py\n\n\n\n\n\n\n\n\n\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n@dataclass\nclass Figure:\n    \"\"\"\n\nFigure objects track the sche\nma supported by the ValidMind API ““”\n    key: str\n    metadata: dict\n    figure: object\n\n    def serialize(self):\n        \"\"\"\nSerializes the Figure to a dictio nary so it can be sent to the API ““” return { “key”: self.key,\n   \"metadata\": self.metadata,\n\n       \"figure\": self.figure,\n        }\n\n\n\n\n\n\n\nserialize()\n\nSerializes the Figure to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/figure.py\n\n\n\n\n\n\n\n\n\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the Figure to a dictio nary so it can be sent to the API ““” return { “key”: self.key,\n   \"metadata\": self.metadata,\n\n       \"figure\": self.figure,\n    }\n\n\n\n\n\n\n\n\n\n\n\nMetric dataclass\n\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\n@dataclass\nclass Metric(TestContextUtils): ““”\nMetric objects track the sche\nma supported by the ValidMind API ““”\n    # Test Context\n    test_context: TestContext\n\n    # Class Variables\n    te\nst_type: ClassVar[str] = “Metric” type: Cl assVar[str] = “” # type of metri c: “training”, “evaluation”, etc.\nscope: ClassVar[str] = “” # sc ope of metric: “training_dataset”\nkey: ClassVar[str] = “” # unique identifer for metric: “accuracy” value_form atter: ClassVar[Optional[str]] = None # “records” or “key_values” de fault_params: ClassVar[dict] = {}\n    # Instance Variables\n    params: dict = None\n\nresult: TestPlanResult = None\n\n    def __post_init__(self):\n        \"\"\"\n        S\net default params if not provided ““”\n      if self.params is None:\nself.params = self.default_params\n    @property\n    def name(self):\n        return self.key\ndef run(self, *args, **kwargs): ““” Run the metric calculation and cache its results ““”\n    raise NotImplementedError\n\n    def cache_results(\n        self,\n        metric_value:\nUnion[dict, list, pd.DataFrame],\nfigures: Optional[object] = None, ): ““” Cache the resu lts of the metric calculation and do any post-processing if needed ““” t est_plan_result = TestPlanResult(\n         metric=MetricResult(\n\n              type=self.type,\n\n            scope=self.scope,\n                key=self.key,\n\n          value=metric_value,\n                valu\ne_formatter=self.value_formatter, ) )\n     # Allow metrics to attac\nh figures to the test plan result if figures: t est_plan_result.figures = figures\nself.result = test_plan_result\n        return self.result\n\n\n\n\n\n\n\n__post_init__()\n\nSet default params if not provided\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n36\n37\n38\n39\n40\n41\n\n\ndef __post_init__(self):\n    \"\"\"\n    S\net default params if not provided ““” if self.params is None:\nself.params = self.default_params\n\n\n\n\n\n\n\n\ncache_results(metric_value,figures=None)\n\nCache the results of the metric calculation and do any post-processing if needed\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\ndef cache_results(\n    self,\n    metric_value:\nUnion[dict, list, pd.DataFrame],\nfigures: Optional[object] = None, ): ““” Cache the resu lts of the metric calculation and do any post-processing if needed ““” t est_plan_result = TestPlanResult( metric=MetricResult( type=self.type, scope=self.scope, key=self.key,\n          value=metric_value,\n            valu\ne_formatter=self.value_formatter, ) )\n    # Allow metrics to attac\nh figures to the test plan result if figures: t est_plan_result.figures = figures\nself.result = test_plan_result\n    return self.result\n\n\n\n\n\n\n\n\nrun(*args,**kwargs)\n\nRun the metric calculation and cache its results\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n47\n48\n49\n50\n51\n\n\ndef run(self, *args, **kwargs): ““” Run the metric calculation and cache its results ““” raise NotImplementedError\n\n\n\n\n\n\n\n\n\n\n\nModel dataclass\n\nModel class wrapper\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n@dataclass\nclass Model:\n    \"\"\"\n    Model class wrapper\n    \"\"\"\n\n    a\nttributes: ModelAttributes = None task: str = None subtask: str = None params: dict = None model_id: str = “main” model: object = None # Trained model instance\n    def serialize(self):\n        \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” return {\n   \"model_id\": self.model_id,\n            \"attri\nbutes”: self.attributes.__dict__,\n           \"task\": self.task,\n\n     \"subtask\": self.subtask,\n\n       \"params\": self.params,\n        }\n\n    de\nf predict(self, *args, **kwargs): ““”\nPredict method for the model. Thi s is a wrapper around the model’s pre dict_proba (for classification) o r predict (for regression) method\nNOTE: This only works for sklear n or xgboost models at the moment ““”\n       predict_fn = getattr(s\nelf.model, “predict_proba”, None)\n     if callable(predict_fn):\n\n        return self.model.pre\ndict_proba(*args, **kwargs)[:, 1] else: return se lf.model.predict(*args, **kwargs)\n    @classmethod\n    de\nf is_supported_model(cls, model): ““” Checks if the model is supported by the API ““” model _class = model.class.name\n        if model_cl\nass not in SUPPORTED_MODEL_TYPES: return False\n        return True\n\n    @classmethod\ndef create_from_dict(cls, dict_): class_field s = {f.name for f in fields(cls)} ret urn Model(**{k: v for k, v in dic t_.items() if k in class_fields})\n\n\n\n\n\n\n\nis_supported_model(model) classmethod\n\nChecks if the model is supported by the API\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n@classmethod\nde\nf is_supported_model(cls, model): ““” Checks if the model is supported by the API ““” model _class = model.class.name\n    if model_cl\nass not in SUPPORTED_MODEL_TYPES: return False\n    return True\n\n\n\n\n\n\n\n\npredict(*args,**kwargs)\n\nPredict method for the model. This is a wrapper around the model's predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\nde\nf predict(self, *args, **kwargs): ““”\nPredict method for the model. Thi s is a wrapper around the model’s pre dict_proba (for classification) o r predict (for regression) method\nNOTE: This only works for sklear n or xgboost models at the moment ““” predict_fn = getattr(s elf.model, “predict_proba”, None) if callable(predict_fn): return self.model.pre dict_proba(*args, **kwargs)[:, 1] else: return se lf.model.predict(*args, **kwargs)\n\n\n\n\n\n\n\n\nserialize()\n\nSerializes the model to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” return {\n   \"model_id\": self.model_id,\n        \"attri\nbutes”: self.attributes.__dict__, “task”: self.task,\n     \"subtask\": self.subtask,\n\n       \"params\": self.params,\n    }\n\n\n\n\n\n\n\n\n\n\n\nModelAttributes dataclass\n\nModel attributes definition\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n@dataclass()\nclass ModelAttributes:\n    \"\"\"\n\n  Model attributes definition\n    \"\"\"\n\n    architecture: str = None\n    framework: str = None\n\nframework_version: str = None\n\n\n\n\n\n\n\n\n\n\n\nThresholdTest dataclass\n\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n@dataclass\nclass\nThresholdTest(TestContextUtils): ““”\nA threshold test is a combinatio n of a metric/plot we track and a\ncorresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails. ““”\n    # Test Context\n    test_context: TestContext\n\n    # Class Variables\n    test_type\n: ClassVar[str] = “ThresholdTest”\n category: ClassVar[str] = \"\"\n    name: ClassVar[str] = \"\"\n    de\nfault_params: ClassVar[dict] = {}\n    # Instance Variables\n    params: dict = None\ntest_results: TestResults = None\n    def __post_init__(self):\n        \"\"\"\n        S\net default params if not provided ““”\n      if self.params is None:\nself.params = self.default_params\ndef run(self, *args, **kwargs): ““” R un the test and cache its results ““”\n    raise NotImplementedError\ndef cache_results(self, results: List[TestResult], passed: bool): ““” Cache the indivi dual results of the threshold tes t as a list of TestResult objects ““” se lf.test_results = TestPlanResult(\n    test_results=TestResults(\n\n      category=self.category,\n\n         test_name=self.name,\n\n          params=self.params,\n\n               passed=passed,\n\n             results=results,\n            )\n        )\n\n     return self.test_results\n\n\n\n\n\n\n\n__post_init__()\n\nSet default params if not provided\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n38\n39\n40\n41\n42\n43\n\n\ndef __post_init__(self):\n    \"\"\"\n    S\net default params if not provided ““” if self.params is None:\nself.params = self.default_params\n\n\n\n\n\n\n\n\ncache_results(results,passed)\n\nCache the individual results of the threshold test as a list of TestResult objects\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\ndef cache_results(self, results: List[TestResult], passed: bool): ““” Cache the indivi dual results of the threshold tes t as a list of TestResult objects ““” se lf.test_results = TestPlanResult(\n    test_results=TestResults(\n\n      category=self.category,\n\n         test_name=self.name,\n\n          params=self.params,\n            passed=passed,\n            results=results,\n        )\n    )\n    return self.test_results\n\n\n\n\n\n\n\n\nrun(*args,**kwargs)\n\nRun the test and cache its results\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n45\n46\n47\n48\n49\n\n\ndef run(self, *args, **kwargs): ““” R un the test and cache its results ““” raise NotImplementedError\n\n\n\n\n\n\n\n\n\n\n\ninit(project,api_key=None,api_secret=None,api_host=None)\n\nInitializes the API client instances and /pings the API to ensure the provided credentials are valid.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n\n\ndef init(project, api_key=None, api_secret=None, api_host=None): ““” Initializes the API cl ient instances and /pings the API to ensure th e provided credentials are valid. ““” global API_HOST\n    ENV_API_K\nEY = os.environ.get(“VM_API_KEY”) ENV_API_SECRET = os.environ.get(“VM_API_SECRET”)\n    vm\n_api_key = api_key or ENV_API_KEY vm_api_secr et = api_secret or ENV_API_SECRET\n    if api_host is not None:\n        API_HOST = api_host\n\n    if vm_api_key\nis None or vm_api_secret is None: raise ValueError(\n \"API key and secret must be\nprovided either as environment va riables or as arguments to init.” )\n  api_session.headers.update(\n        {\n\n     \"X-API-KEY\": vm_api_key,\n“X-API-SECRET”: vm_api_secret,\n   \"X-PROJECT-CUID\": project,\n        }\n    )\n\n    return __ping()\n\n\n\n\n\n\n\n\ninit_dataset(dataset,type='training',options=None,targets=None,target_column=None,class_labels=None)\n\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n:param pd.DataFrame dataset: We only support Pandas DataFrames at the moment :param str type: The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic :param dict options: A dictionary of options for the dataset :param vm.vm.DatasetTargets targets: A list of target variables :param str target_column: The name of the target column in the dataset :param dict class_labels: A list of class labels for classification problems\nSource code in validmind/client.py\n\n\n\n\n\n\n\n\n\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\ndef init_dataset(\n    dataset,\n    type=\"training\",\n    options=None,\n    targets=None,\n    target_column=None,\n    class_labels=None,\n):\n    \"\"\"\n    Ini\ntializes a VM Dataset, which can then be passed to other functions that ca n perform additional analysis and tests on the data. This function also e nsures we are reading a valid dat aset type. We only support Pandas DataFrames at the moment.\n    :param pd.\nDataFrame dataset: We only suppor t Pandas DataFrames at the moment :param str type: The dataset split type is necessary for mapping and relating multiple data sets together. Can be one of trai ning, validation, test or generic\n  :param dict options: A dict\nionary of options for the dataset\n:param vm.vm.DatasetTargets ta rgets: A list of target variables :par am str target_column: The name of the target column in the dataset :param dic t class_labels: A list of class l abels for classification problems ““” dataset_c lass = dataset.__class__.__name__\n    # TODO\n: when we accept numpy datasets w e can convert them to/from pandas\nif dataset_class == “DataFrame”: pri nt(“Pandas dataset detected. Init ializing VM Dataset instance…”) vm_datase t = Dataset.init_from_pd_dataset(\n        dataset, options, tar\ngets, target_column, class_labels ) else: rai se ValueError(“Only Pandas datase ts are supported at the moment.”)\n    vm_dataset.type = type\n\n    return vm_dataset\n\n\n\n\n\n\n\n\ninit_model(model)\n\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n:param model: A trained model instance\nSource code in validmind/client.py\n\n\n\n\n\n\n\n\n\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\ndef init_model(model):\n    \"\"\"\n    I\nnitializes a VM Model, which can then be passed to other functions that ca n perform additional analysis and tests on the data. This function also ensures we ar e reading a supported model type.\n    :para\nm model: A trained model instance ““”\n    if not\nModel.is_supported_model(model): raise ValueError(\n       \"Model type {} is not\nsupported at the moment.”.format(\n     model.__class__.__name__\n            )\n        )\n\n\n   vm_model = Model(model=mod\nel, attributes=ModelAttributes())\n    return vm_model\n\n\n\n\n\n\n\n\nlog_dataset(vm_dataset)\n\nLogs metadata and statistics about a dataset to ValidMind API.\n:param dataset: A VM dataset object :param dataset_type: The type of dataset. Can be one of \"training\", \"test\", or \"validation\". :param dataset_options: Additional dataset options for analysis :param dataset_targets: A list of targets for the dataset. :param features: Optional. A list of features metadata. :type dataset_targets: validmind.DatasetTargets, optional\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\ndef log_dataset(vm_dataset):\n    \"\"\"\n\nLogs metadata and statistics\nabout a dataset to ValidMind API.\n    :p\naram dataset: A VM dataset object\n   :param dataset_type: The t\nype of dataset. Can be one of “tr aining”, “test”, or “validation”.\n:param dataset_options: Additi onal dataset options for analysis :param dataset_targets: A list of targets for the dataset. :param features: Optio nal. A list of features metadata. :type dataset_targets: v alidmind.DatasetTargets, optional ““”\npayload = json.dumps(vm_datas\net.serialize(), cls=NumpyEncoder) r = api_session.post(\n   f\"{API_HOST}/log_dataset\",\n        data=payload,\n        headers={\"Co\nntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could n\not log dataset to ValidMind API”)\n      raise Exception(r.text)\n\n\n print(\"Successfully logged d\nataset metadata and statistics.”)\n    return vm_dataset\n\n\n\n\n\n\n\n\nlog_figure(data_or_path,key,metadata,run_cuid=None)\n\nLogs a figure\n:param data_or_path: the path of the image or the data of the plot :param key: identifier of the figure :param metadata: python data structure :param run_cuid: run cuid from start_run\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n\n\ndef log_figure(data_or_pat\nh, key, metadata, run_cuid=None): ““” Logs a figure\n:param data_or_path: the path of the image or the data of the plot :pa ram key: identifier of the figure :para m metadata: python data structure :param run_cuid: run cuid from start_run ““” if not run_cuid: run _cuid = _get_or_create_run_cuid()\n    url = f\"{API_HOST\n}/log_figure?run_cuid={run_cuid}”\nif isinstance(data_or_path, str): type_ = “file_path” _, extension = os.path.splitext(data_or_path)\nfiles = {“image”: (f”{key}{extens ion}“, open(data_or_path,”rb”))}\n elif is_matplotlib_typename(\nget_full_typename(data_or_path)): type_ = “plot” buffer = BytesIO() data_or_path.sav efig(buffer, bbox_inches=“tight”) buffer.seek(0) files = {“image”: (f” {key}.png”, buffer, “image/png”)} else: raise ValueError( f”dat a_or_path type not supported: {ge t_full_typename(data_or_path)}. ”\n       f\"Available supported\ntypes: string path or matplotlib” )\n    try:\n        met\nadata_json = json.dumps(metadata) except TypeError: raise\n    res = api_session.post(\n        url, files=\nfiles, data={“key”: key, “type”: type_, “metadata”: metadata_json} ) return res.json()\n\n\n\n\n\n\n\n\nlog_metadata(content_id,text=None,extra_json=None)\n\nLogs free-form metadata to ValidMind API.\n:param content_id: Unique content identifier for the metadata :param text: Free-form text to assign to the metadata :param extra_json: Free-form key-value pairs to assign to the metadata\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n\n\ndef log_metadata(content\n_id, text=None, extra_json=None): ““” Logs fre e-form metadata to ValidMind API.\n :param content_id: Unique co\nntent identifier for the metadata :param text: Free-fo rm text to assign to the metadata :par am extra_json: Free-form key-valu e pairs to assign to the metadata ““” metadata_dict = {\n    \"content_id\": content_id,\n    }\n\n    if text is not None:\n\n metadata_dict[\"text\"] = text\n\n   if extra_json is not None:\n        m\netadata_dict[“json”] = extra_json\n    r = api_session.post(\n\n  f\"{API_HOST}/log_metadata\",\n        data=json.dumps(\nmetadata_dict, cls=NumpyEncoder), headers={“Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could no\nt log metadata to ValidMind API”)\n      raise Exception(r.text)\n\n    prin\nt(“Successfully logged metadata”)\n    return True\n\n\n\n\n\n\n\n\nlog_metrics(metrics,run_cuid=None)\n\nLogs metrics to ValidMind API.\n:param metrics: A list of Metric objects. :param run_cuid: The run CUID. If not provided, a new run will be created.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n\n\ndef log\n_metrics(metrics, run_cuid=None): ““”\nLogs metrics to ValidMind API.\n    :param m\netrics: A list of Metric objects. :param r un_cuid: The run CUID. If not pro vided, a new run will be created. ““” if run_cuid is None:\n       run_cuid = start_run()\n\n    serialized_metrics =\n[m.serialize() for m in metrics]\n    r = api_session.post(\n        f\"{API_HOST}/\nlog_metrics?run_cuid={run_cuid}“, data=json.dumps(seria lized_metrics, cls=NumpyEncoder), headers={”Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could n\not log metrics to ValidMind API”)\n      raise Exception(r.text)\n\n    pri\nnt(“Successfully logged metrics”)\n    return True\n\n\n\n\n\n\n\n\nlog_model(vm_model)\n\nLogs model metadata and hyperparameters to ValidMind API. :param vm_model: A ValidMind Model wrapper instance.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n\n\ndef log_model(vm_model):\n    \"\"\"\n    Logs model metadata and\nhyperparameters to ValidMind API. :param vm_model: A ValidMind Model wrapper instance. ““” r = api_session.post(\n     f\"{API_HOST}/log_model\",\n\n      data=json.dumps(vm_mode\nl.serialize(), cls=NumpyEncoder), headers={“Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could\nnot log model to ValidMind API”)\n      raise Exception(r.text)\n\n    return True\n\n\n\n\n\n\n\n\nlog_test_results(results,run_cuid=None,dataset_type='training')\n\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n:param results: A list of TestResults objects :param run_cuid: The run CUID. If not provided, a new run will be created.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n\n\ndef\nlog_test_results(results, run_cui d=None, dataset_type=“training”): ““” Logs test results inf ormation. This method will be cal led automatically be any function running tests but can al so be called directly if the user wants to run tests on their own.\n    :param resul\nts: A list of TestResults objects :param r un_cuid: The run CUID. If not pro vided, a new run will be created. ““” if run_cuid is None:\n       run_cuid = start_run()\n# TBD - parallelize API requests for result in results: log_test_resul t(result, run_cuid, dataset_type)\n    return True"
  },
  {
    "objectID": "python-library/index.html#validmind.Dataset",
    "href": "python-library/index.html#validmind.Dataset",
    "title": "ValidMind",
    "section": "Dataset dataclass",
    "text": "Dataset dataclass\n\nModel class wrapper\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n\n\n@dataclass()\nclass Dataset:\n    \"\"\"\n    Model class wrapper\n    \"\"\"\n\n    raw_dataset: object\n    fields: list\n    variables: list\n    sample: list\n    shape: dict\ncorrelation_matrix: object = None correlations: dict = None type: str = None options: dict = None statistics: dict = None\n    # Specif\ny targets via DatasetTargets or v ia target_column and class_labels targets: dict = None target_column: str = “” class_labels: dict = None\n    __feature_lookup: d\nict = field(default_factory=dict)\n__transformed_df: object = None\n    def __post_init__(self):\n        \"\"\"\n        Set target_column and\nclass_labels from DatasetTargets ““” if self.targets: self.target_co lumn = self.targets.target_column self.class_l abels = self.targets.class_labels\n    @property\n    def x(self):\n        \"\"\"\nReturns the dataset’s features ““”\n      return self.raw_dataset\n.drop(self.target_column, axis=1)\n    @property\n    def y(self):\n        \"\"\"\n        Re\nturns the dataset’s target column ““” return sel f.raw_dataset[self.target_column]\n    def get\n_feature_by_id(self, feature_id): ““”\n   Returns the feature with t\nhe given id. We also build a lazy\nlookup cache in case the same fea ture is requested multiple times. ““” if feature _id not in self.__feature_lookup:\n  for feature in self.fields:\nif feature[“id”] == feature_id:\n                   self.__fea\nture_lookup[feature_id] = feature\n               return feature\nraise ValueError(f”Feature with id {feature_id} does not exist”)\n        return\nself.__feature_lookup[feature_id]\n    def ge\nt_feature_type(self, feature_id): ““” Returns the type of the feature with the given id ““” feature = s elf.get_feature_by_id(feature_id)\n       return feature[\"type\"]\n\n    def serialize(self):\n        \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” dataset_dict = {\n         \"shape\": self.shape,\n\n           \"type\": self.type,\n        }\n\n        # Data\nset with no targets can be logged if self.targets: dataset_dict[” targets”] = self.targets.__dict__ else:\n  dataset_dict[\"targets\"] = {\n                \"ta\nrget_column”: self.target_column, ” class_labels”: self.class_labels, }\n        return dataset_dict\n\n    def describe(self):\n        \"\"\"\n\n Extracts descriptive statist\nics for each field in the dataset ““” transfor med_df = self.transformed_dataset\n for ds_field in self.fields:\n            describe_datase\nt_field(transformed_df, ds_field)\n  def get_correlations(self):\n        \"\"\"\n        Extracts correlati\nons for each field in the dataset ““” # Ignore field s that have very high cardinality\n  fields_for_correlation = []\n\n for ds_field in self.fields:\n            if\n“statistics” in ds_field and “dis tinct” in ds_field[“statistics”]: if ds_field [“statistics”][“distinct”] < 0.1:\n                 fields_for_c\norrelation.append(ds_field[“id”])\n        self.c\norrelation_matrix = associations( self.transformed _dataset[fields_for_correlation],\n           compute_only=True,\n            plot=False,\n        )[\"corr\"]\n\n        # Transform to the\ncurrent format expected by the UI self.correlations = [ [ {\n                \"field\": key,\n\n              \"value\": value,\n                }\n                for key,\nvalue in correlation_row.items() ] for co rrelation_row in self.correlation _matrix.to_dict(orient=“records”) ]\n    def get_c\norrelation_plots(self, n_top=15): ““” Ex tracts correlation plots for the n_top correlations in the dataset ““”\n   correlation_plots = genera\nte_correlation_plots(self, n_top)\n     return correlation_plots\n\n    @property\n    def transformed_da\ntaset(self, force_refresh=False): ““” Ret urns a transformed dataset that u ses the features from vm_dataset.\nSome of the features in vm_datase t are of type Dummy so we need to\nreverse the one hot encoding and drop the individual dummy columns ““”\nif self.__transformed_df is not None and force_refresh is False:\n return self.__transformed_df\n\n        # Get the list o\nf features that are of type Dummy\ndataset_options = self.options dummy_variables = ( dat aset_options.get(“dummy_variables”, []) if dataset_options else [] ) # Exc lude columns that have prefixes t hat are in the dummy feature list\n       dummy_column_names = [\n            column_name\n            for column\n_name in self.raw_dataset.columns if any( colum n_name.startswith(dummy_variable) for dummy_variable in dummy_variables ) ]\ntransformed_df = self.raw_dataset .drop(dummy_column_names, axis=1)\n      # Add reversed dummy fe\natures to the transformed dataset for d ummy_variable in dummy_variables:\ncolumns_with_dummy_prefix = [\n                col\n                fo\nr col in self.raw_dataset.columns\nif col.startswith(dummy_variable) ] t ransformed_df[dummy_variable] = ( self.raw_d ataset[columns_with_dummy_prefix]\n              .idxmax(axis=1)\n\n            .replace(f\"{dummy\n_variable}[-_:]“,”“, regex=True) )\n        return transformed_df\n\n    @classmethod\ndef create_from_dict(cls, dict_): class_field s = {f.name for f in fields(cls)} retur n Dataset(**{k: v for k, v in dic t_.items() if k in class_fields})\n    # TODO: Accept var\niable descriptions from framework # TODO: Acc ept type overrides from framework @classmethod def init_from_pd_dataset( cls, df, options=None, targets=None, targ et_column=None, class_labels=None ): pr int(“Inferring dataset types…”)\n   vm_dataset_variables = par\nse_dataset_variables(df, options)\n        shape = {\n\n         \"rows\": df.shape[0],\n\n      \"columns\": df.shape[1],\n        }\n        df_head = df\n.head().to_dict(orient=“records”) df_tail = df .tail().to_dict(orient=“records”)\n        # TODO: validate wi\nth target_column and class_labels if targets: validat e_pd_dataset_targets(df, targets)\n        return Dataset(\n            raw_dataset=df,\n            fields=vm_d\nataset_variables, # TODO - depre cate naming in favor of variables\nvariables=vm_dataset_variables, sample=[ {\n                \"id\": \"head\",\n\n             \"data\": df_head,\n                },\n                {\n\n                \"id\": \"tail\",\n\n             \"data\": df_tail,\n                },\n            ],\n            shape=shape,\n            targets=targets,\n\n target_column=target_column,\n\n   class_labels=class_labels,\n            options=options,\n        )\n\n\n\n\n\n\n\ntransformed_dataset property\n\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\n\n\nx property\n\nReturns the dataset's features\n\n\n\ny property\n\nReturns the dataset's target column\n\n\n\n__post_init__()\n\nSet target_column and class_labels from DatasetTargets\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n52\n53\n54\n55\n56\n57\n58\n\n\ndef __post_init__(self):\n    \"\"\"\n    Set target_column and\nclass_labels from DatasetTargets ““” if self.targets: self.target_co lumn = self.targets.target_column self.class_l abels = self.targets.class_labels\n\n\n\n\n\n\n\n\ndescribe()\n\nExtracts descriptive statistics for each field in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n115\n116\n117\n118\n119\n120\n121\n122\n\n\ndef describe(self):\n    \"\"\"\n\n Extracts descriptive statist\nics for each field in the dataset ““” transfor med_df = self.transformed_dataset\n for ds_field in self.fields:\n        describe_datase\nt_field(transformed_df, ds_field)\n\n\n\n\n\n\n\n\nget_correlation_plots(n_top=15)\n\nExtracts correlation plots for the n_top correlations in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n153\n154\n155\n156\n157\n158\n\n\ndef get_c\norrelation_plots(self, n_top=15): ““” Ex tracts correlation plots for the n_top correlations in the dataset ““”\n   correlation_plots = genera\nte_correlation_plots(self, n_top) return correlation_plots\n\n\n\n\n\n\n\n\nget_correlations()\n\nExtracts correlations for each field in the dataset\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n\n\ndef get_correlations(self):\n    \"\"\"\n    Extracts correlati\nons for each field in the dataset ““” # Ignore field s that have very high cardinality\n  fields_for_correlation = []\n\n for ds_field in self.fields:\n        if\n“statistics” in ds_field and “dis tinct” in ds_field[“statistics”]: if ds_field [“statistics”][“distinct”] < 0.1: fields_for_c orrelation.append(ds_field[“id”])\n    self.c\norrelation_matrix = associations( self.transformed _dataset[fields_for_correlation], compute_only=True, plot=False, )[“corr”]\n    # Transform to the\ncurrent format expected by the UI self.correlations = [ [ { “field”: key,\n              \"value\": value,\n            }\n            for key,\nvalue in correlation_row.items() ] for co rrelation_row in self.correlation _matrix.to_dict(orient=“records”) ]\n\n\n\n\n\n\n\n\nget_feature_by_id(feature_id)\n\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\ndef get\n_feature_by_id(self, feature_id): ““”\n   Returns the feature with t\nhe given id. We also build a lazy\nlookup cache in case the same fea ture is requested multiple times. ““” if feature _id not in self.__feature_lookup:\n  for feature in self.fields:\nif feature[“id”] == feature_id: self.__fea ture_lookup[feature_id] = feature\n               return feature\nraise ValueError(f”Feature with id {feature_id} does not exist”)\n    return\nself.__feature_lookup[feature_id]\n\n\n\n\n\n\n\n\nget_feature_type(feature_id)\n\nReturns the type of the feature with the given id\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n88\n89\n90\n91\n92\n93\n\n\ndef ge\nt_feature_type(self, feature_id): ““” Returns the type of the feature with the given id ““” feature = s elf.get_feature_by_id(feature_id) return feature[“type”]\n\n\n\n\n\n\n\n\nserialize()\n\nSerializes the model to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” dataset_dict = { “shape”: self.shape, “type”: self.type, }\n    # Data\nset with no targets can be logged if self.targets: dataset_dict[” targets”] = self.targets.__dict__ else:\n  dataset_dict[\"targets\"] = {\n            \"ta\nrget_column”: self.target_column, ” class_labels”: self.class_labels, }\n    return dataset_dict"
  },
  {
    "objectID": "python-library/index.html#validmind.DatasetTargets",
    "href": "python-library/index.html#validmind.DatasetTargets",
    "title": "ValidMind",
    "section": "DatasetTargets dataclass",
    "text": "DatasetTargets dataclass\n\nDataset targets definition\nSource code in validmind/vm_models/dataset.py\n\n\n\n\n\n\n\n\n\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n@dataclass()\nclass DatasetTargets:\n    \"\"\"\n\n   Dataset targets definition\n    \"\"\"\n\n    target_column: str\n    description: str = None\n    class_labels: dict = None"
  },
  {
    "objectID": "python-library/index.html#validmind.Figure",
    "href": "python-library/index.html#validmind.Figure",
    "title": "ValidMind",
    "section": "Figure dataclass",
    "text": "Figure dataclass\n\nFigure objects track the schema supported by the ValidMind API\nSource code in validmind/vm_models/figure.py\n\n\n\n\n\n\n\n\n\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n@dataclass\nclass Figure:\n    \"\"\"\n\nFigure objects track the sche\nma supported by the ValidMind API ““”\n    key: str\n    metadata: dict\n    figure: object\n\n    def serialize(self):\n        \"\"\"\nSerializes the Figure to a dictio nary so it can be sent to the API ““” return { “key”: self.key,\n   \"metadata\": self.metadata,\n\n       \"figure\": self.figure,\n        }\n\n\n\n\n\n\n\nserialize()\n\nSerializes the Figure to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/figure.py\n\n\n\n\n\n\n\n\n\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the Figure to a dictio nary so it can be sent to the API ““” return { “key”: self.key,\n   \"metadata\": self.metadata,\n\n       \"figure\": self.figure,\n    }"
  },
  {
    "objectID": "python-library/index.html#validmind.Metric",
    "href": "python-library/index.html#validmind.Metric",
    "title": "ValidMind",
    "section": "Metric dataclass",
    "text": "Metric dataclass\n\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\n@dataclass\nclass Metric(TestContextUtils): ““”\nMetric objects track the sche\nma supported by the ValidMind API ““”\n    # Test Context\n    test_context: TestContext\n\n    # Class Variables\n    te\nst_type: ClassVar[str] = “Metric” type: Cl assVar[str] = “” # type of metri c: “training”, “evaluation”, etc.\nscope: ClassVar[str] = “” # sc ope of metric: “training_dataset”\nkey: ClassVar[str] = “” # unique identifer for metric: “accuracy” value_form atter: ClassVar[Optional[str]] = None # “records” or “key_values” de fault_params: ClassVar[dict] = {}\n    # Instance Variables\n    params: dict = None\n\nresult: TestPlanResult = None\n\n    def __post_init__(self):\n        \"\"\"\n        S\net default params if not provided ““”\n      if self.params is None:\nself.params = self.default_params\n    @property\n    def name(self):\n        return self.key\ndef run(self, *args, **kwargs): ““” Run the metric calculation and cache its results ““”\n    raise NotImplementedError\n\n    def cache_results(\n        self,\n        metric_value:\nUnion[dict, list, pd.DataFrame],\nfigures: Optional[object] = None, ): ““” Cache the resu lts of the metric calculation and do any post-processing if needed ““” t est_plan_result = TestPlanResult(\n         metric=MetricResult(\n\n              type=self.type,\n\n            scope=self.scope,\n                key=self.key,\n\n          value=metric_value,\n                valu\ne_formatter=self.value_formatter, ) )\n     # Allow metrics to attac\nh figures to the test plan result if figures: t est_plan_result.figures = figures\nself.result = test_plan_result\n        return self.result\n\n\n\n\n\n\n\n__post_init__()\n\nSet default params if not provided\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n36\n37\n38\n39\n40\n41\n\n\ndef __post_init__(self):\n    \"\"\"\n    S\net default params if not provided ““” if self.params is None:\nself.params = self.default_params\n\n\n\n\n\n\n\n\ncache_results(metric_value,figures=None)\n\nCache the results of the metric calculation and do any post-processing if needed\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\ndef cache_results(\n    self,\n    metric_value:\nUnion[dict, list, pd.DataFrame],\nfigures: Optional[object] = None, ): ““” Cache the resu lts of the metric calculation and do any post-processing if needed ““” t est_plan_result = TestPlanResult( metric=MetricResult( type=self.type, scope=self.scope, key=self.key,\n          value=metric_value,\n            valu\ne_formatter=self.value_formatter, ) )\n    # Allow metrics to attac\nh figures to the test plan result if figures: t est_plan_result.figures = figures\nself.result = test_plan_result\n    return self.result\n\n\n\n\n\n\n\n\nrun(*args,**kwargs)\n\nRun the metric calculation and cache its results\nSource code in validmind/vm_models/metric.py\n\n\n\n\n\n\n\n\n\n47\n48\n49\n50\n51\n\n\ndef run(self, *args, **kwargs): ““” Run the metric calculation and cache its results ““” raise NotImplementedError"
  },
  {
    "objectID": "python-library/index.html#validmind.Model",
    "href": "python-library/index.html#validmind.Model",
    "title": "ValidMind",
    "section": "Model dataclass",
    "text": "Model dataclass\n\nModel class wrapper\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n@dataclass\nclass Model:\n    \"\"\"\n    Model class wrapper\n    \"\"\"\n\n    a\nttributes: ModelAttributes = None task: str = None subtask: str = None params: dict = None model_id: str = “main” model: object = None # Trained model instance\n    def serialize(self):\n        \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” return {\n   \"model_id\": self.model_id,\n            \"attri\nbutes”: self.attributes.__dict__,\n           \"task\": self.task,\n\n     \"subtask\": self.subtask,\n\n       \"params\": self.params,\n        }\n\n    de\nf predict(self, *args, **kwargs): ““”\nPredict method for the model. Thi s is a wrapper around the model’s pre dict_proba (for classification) o r predict (for regression) method\nNOTE: This only works for sklear n or xgboost models at the moment ““”\n       predict_fn = getattr(s\nelf.model, “predict_proba”, None)\n     if callable(predict_fn):\n\n        return self.model.pre\ndict_proba(*args, **kwargs)[:, 1] else: return se lf.model.predict(*args, **kwargs)\n    @classmethod\n    de\nf is_supported_model(cls, model): ““” Checks if the model is supported by the API ““” model _class = model.class.name\n        if model_cl\nass not in SUPPORTED_MODEL_TYPES: return False\n        return True\n\n    @classmethod\ndef create_from_dict(cls, dict_): class_field s = {f.name for f in fields(cls)} ret urn Model(**{k: v for k, v in dic t_.items() if k in class_fields})\n\n\n\n\n\n\n\nis_supported_model(model) classmethod\n\nChecks if the model is supported by the API\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n@classmethod\nde\nf is_supported_model(cls, model): ““” Checks if the model is supported by the API ““” model _class = model.class.name\n    if model_cl\nass not in SUPPORTED_MODEL_TYPES: return False\n    return True\n\n\n\n\n\n\n\n\npredict(*args,**kwargs)\n\nPredict method for the model. This is a wrapper around the model's predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\nde\nf predict(self, *args, **kwargs): ““”\nPredict method for the model. Thi s is a wrapper around the model’s pre dict_proba (for classification) o r predict (for regression) method\nNOTE: This only works for sklear n or xgboost models at the moment ““” predict_fn = getattr(s elf.model, “predict_proba”, None) if callable(predict_fn): return self.model.pre dict_proba(*args, **kwargs)[:, 1] else: return se lf.model.predict(*args, **kwargs)\n\n\n\n\n\n\n\n\nserialize()\n\nSerializes the model to a dictionary so it can be sent to the API\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\ndef serialize(self):\n    \"\"\"\nSerializes the model to a dictio nary so it can be sent to the API ““” return {\n   \"model_id\": self.model_id,\n        \"attri\nbutes”: self.attributes.__dict__, “task”: self.task,\n     \"subtask\": self.subtask,\n\n       \"params\": self.params,\n    }"
  },
  {
    "objectID": "python-library/index.html#validmind.ModelAttributes",
    "href": "python-library/index.html#validmind.ModelAttributes",
    "title": "ValidMind",
    "section": "ModelAttributes dataclass",
    "text": "ModelAttributes dataclass\n\nModel attributes definition\nSource code in validmind/vm_models/model.py\n\n\n\n\n\n\n\n\n\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n@dataclass()\nclass ModelAttributes:\n    \"\"\"\n\n  Model attributes definition\n    \"\"\"\n\n    architecture: str = None\n    framework: str = None\n\nframework_version: str = None"
  },
  {
    "objectID": "python-library/index.html#validmind.ThresholdTest",
    "href": "python-library/index.html#validmind.ThresholdTest",
    "title": "ValidMind",
    "section": "ThresholdTest dataclass",
    "text": "ThresholdTest dataclass\n\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n@dataclass\nclass\nThresholdTest(TestContextUtils): ““”\nA threshold test is a combinatio n of a metric/plot we track and a\ncorresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails. ““”\n    # Test Context\n    test_context: TestContext\n\n    # Class Variables\n    test_type\n: ClassVar[str] = “ThresholdTest”\n category: ClassVar[str] = \"\"\n    name: ClassVar[str] = \"\"\n    de\nfault_params: ClassVar[dict] = {}\n    # Instance Variables\n    params: dict = None\ntest_results: TestResults = None\n    def __post_init__(self):\n        \"\"\"\n        S\net default params if not provided ““”\n      if self.params is None:\nself.params = self.default_params\ndef run(self, *args, **kwargs): ““” R un the test and cache its results ““”\n    raise NotImplementedError\ndef cache_results(self, results: List[TestResult], passed: bool): ““” Cache the indivi dual results of the threshold tes t as a list of TestResult objects ““” se lf.test_results = TestPlanResult(\n    test_results=TestResults(\n\n      category=self.category,\n\n         test_name=self.name,\n\n          params=self.params,\n\n               passed=passed,\n\n             results=results,\n            )\n        )\n\n     return self.test_results\n\n\n\n\n\n\n\n__post_init__()\n\nSet default params if not provided\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n38\n39\n40\n41\n42\n43\n\n\ndef __post_init__(self):\n    \"\"\"\n    S\net default params if not provided ““” if self.params is None:\nself.params = self.default_params\n\n\n\n\n\n\n\n\ncache_results(results,passed)\n\nCache the individual results of the threshold test as a list of TestResult objects\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\ndef cache_results(self, results: List[TestResult], passed: bool): ““” Cache the indivi dual results of the threshold tes t as a list of TestResult objects ““” se lf.test_results = TestPlanResult(\n    test_results=TestResults(\n\n      category=self.category,\n\n         test_name=self.name,\n\n          params=self.params,\n            passed=passed,\n            results=results,\n        )\n    )\n    return self.test_results\n\n\n\n\n\n\n\n\nrun(*args,**kwargs)\n\nRun the test and cache its results\nSource code in validmind/vm_models/threshold_test.py\n\n\n\n\n\n\n\n\n\n45\n46\n47\n48\n49\n\n\ndef run(self, *args, **kwargs): ““” R un the test and cache its results ““” raise NotImplementedError"
  },
  {
    "objectID": "python-library/index.html#validmind.init",
    "href": "python-library/index.html#validmind.init",
    "title": "ValidMind",
    "section": "init(project,api_key=None,api_secret=None,api_host=None)",
    "text": "init(project,api_key=None,api_secret=None,api_host=None)\n\nInitializes the API client instances and /pings the API to ensure the provided credentials are valid.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n\n\ndef init(project, api_key=None, api_secret=None, api_host=None): ““” Initializes the API cl ient instances and /pings the API to ensure th e provided credentials are valid. ““” global API_HOST\n    ENV_API_K\nEY = os.environ.get(“VM_API_KEY”) ENV_API_SECRET = os.environ.get(“VM_API_SECRET”)\n    vm\n_api_key = api_key or ENV_API_KEY vm_api_secr et = api_secret or ENV_API_SECRET\n    if api_host is not None:\n        API_HOST = api_host\n\n    if vm_api_key\nis None or vm_api_secret is None: raise ValueError(\n \"API key and secret must be\nprovided either as environment va riables or as arguments to init.” )\n  api_session.headers.update(\n        {\n\n     \"X-API-KEY\": vm_api_key,\n“X-API-SECRET”: vm_api_secret,\n   \"X-PROJECT-CUID\": project,\n        }\n    )\n\n    return __ping()"
  },
  {
    "objectID": "python-library/index.html#validmind.init_dataset",
    "href": "python-library/index.html#validmind.init_dataset",
    "title": "ValidMind",
    "section": "init_dataset(dataset,type='training',options=None,targets=None,target_column=None,class_labels=None)",
    "text": "init_dataset(dataset,type='training',options=None,targets=None,target_column=None,class_labels=None)\n\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n:param pd.DataFrame dataset: We only support Pandas DataFrames at the moment :param str type: The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic :param dict options: A dictionary of options for the dataset :param vm.vm.DatasetTargets targets: A list of target variables :param str target_column: The name of the target column in the dataset :param dict class_labels: A list of class labels for classification problems\nSource code in validmind/client.py\n\n\n\n\n\n\n\n\n\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\ndef init_dataset(\n    dataset,\n    type=\"training\",\n    options=None,\n    targets=None,\n    target_column=None,\n    class_labels=None,\n):\n    \"\"\"\n    Ini\ntializes a VM Dataset, which can then be passed to other functions that ca n perform additional analysis and tests on the data. This function also e nsures we are reading a valid dat aset type. We only support Pandas DataFrames at the moment.\n    :param pd.\nDataFrame dataset: We only suppor t Pandas DataFrames at the moment :param str type: The dataset split type is necessary for mapping and relating multiple data sets together. Can be one of trai ning, validation, test or generic\n  :param dict options: A dict\nionary of options for the dataset\n:param vm.vm.DatasetTargets ta rgets: A list of target variables :par am str target_column: The name of the target column in the dataset :param dic t class_labels: A list of class l abels for classification problems ““” dataset_c lass = dataset.__class__.__name__\n    # TODO\n: when we accept numpy datasets w e can convert them to/from pandas\nif dataset_class == “DataFrame”: pri nt(“Pandas dataset detected. Init ializing VM Dataset instance…”) vm_datase t = Dataset.init_from_pd_dataset(\n        dataset, options, tar\ngets, target_column, class_labels ) else: rai se ValueError(“Only Pandas datase ts are supported at the moment.”)\n    vm_dataset.type = type\n\n    return vm_dataset"
  },
  {
    "objectID": "python-library/index.html#validmind.init_model",
    "href": "python-library/index.html#validmind.init_model",
    "title": "ValidMind",
    "section": "init_model(model)",
    "text": "init_model(model)\n\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n:param model: A trained model instance\nSource code in validmind/client.py\n\n\n\n\n\n\n\n\n\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\ndef init_model(model):\n    \"\"\"\n    I\nnitializes a VM Model, which can then be passed to other functions that ca n perform additional analysis and tests on the data. This function also ensures we ar e reading a supported model type.\n    :para\nm model: A trained model instance ““”\n    if not\nModel.is_supported_model(model): raise ValueError(\n       \"Model type {} is not\nsupported at the moment.”.format(\n     model.__class__.__name__\n            )\n        )\n\n\n   vm_model = Model(model=mod\nel, attributes=ModelAttributes())\n    return vm_model"
  },
  {
    "objectID": "python-library/index.html#validmind.log_dataset",
    "href": "python-library/index.html#validmind.log_dataset",
    "title": "ValidMind",
    "section": "log_dataset(vm_dataset)",
    "text": "log_dataset(vm_dataset)\n\nLogs metadata and statistics about a dataset to ValidMind API.\n:param dataset: A VM dataset object :param dataset_type: The type of dataset. Can be one of \"training\", \"test\", or \"validation\". :param dataset_options: Additional dataset options for analysis :param dataset_targets: A list of targets for the dataset. :param features: Optional. A list of features metadata. :type dataset_targets: validmind.DatasetTargets, optional\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\ndef log_dataset(vm_dataset):\n    \"\"\"\n\nLogs metadata and statistics\nabout a dataset to ValidMind API.\n    :p\naram dataset: A VM dataset object\n   :param dataset_type: The t\nype of dataset. Can be one of “tr aining”, “test”, or “validation”.\n:param dataset_options: Additi onal dataset options for analysis :param dataset_targets: A list of targets for the dataset. :param features: Optio nal. A list of features metadata. :type dataset_targets: v alidmind.DatasetTargets, optional ““”\npayload = json.dumps(vm_datas\net.serialize(), cls=NumpyEncoder) r = api_session.post(\n   f\"{API_HOST}/log_dataset\",\n        data=payload,\n        headers={\"Co\nntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could n\not log dataset to ValidMind API”)\n      raise Exception(r.text)\n\n\n print(\"Successfully logged d\nataset metadata and statistics.”)\n    return vm_dataset"
  },
  {
    "objectID": "python-library/index.html#validmind.log_figure",
    "href": "python-library/index.html#validmind.log_figure",
    "title": "ValidMind",
    "section": "log_figure(data_or_path,key,metadata,run_cuid=None)",
    "text": "log_figure(data_or_path,key,metadata,run_cuid=None)\n\nLogs a figure\n:param data_or_path: the path of the image or the data of the plot :param key: identifier of the figure :param metadata: python data structure :param run_cuid: run cuid from start_run\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n\n\ndef log_figure(data_or_pat\nh, key, metadata, run_cuid=None): ““” Logs a figure\n:param data_or_path: the path of the image or the data of the plot :pa ram key: identifier of the figure :para m metadata: python data structure :param run_cuid: run cuid from start_run ““” if not run_cuid: run _cuid = _get_or_create_run_cuid()\n    url = f\"{API_HOST\n}/log_figure?run_cuid={run_cuid}”\nif isinstance(data_or_path, str): type_ = “file_path” _, extension = os.path.splitext(data_or_path)\nfiles = {“image”: (f”{key}{extens ion}“, open(data_or_path,”rb”))}\n elif is_matplotlib_typename(\nget_full_typename(data_or_path)): type_ = “plot” buffer = BytesIO() data_or_path.sav efig(buffer, bbox_inches=“tight”) buffer.seek(0) files = {“image”: (f” {key}.png”, buffer, “image/png”)} else: raise ValueError( f”dat a_or_path type not supported: {ge t_full_typename(data_or_path)}. ”\n       f\"Available supported\ntypes: string path or matplotlib” )\n    try:\n        met\nadata_json = json.dumps(metadata) except TypeError: raise\n    res = api_session.post(\n        url, files=\nfiles, data={“key”: key, “type”: type_, “metadata”: metadata_json} ) return res.json()"
  },
  {
    "objectID": "python-library/index.html#validmind.log_metadata",
    "href": "python-library/index.html#validmind.log_metadata",
    "title": "ValidMind",
    "section": "log_metadata(content_id,text=None,extra_json=None)",
    "text": "log_metadata(content_id,text=None,extra_json=None)\n\nLogs free-form metadata to ValidMind API.\n:param content_id: Unique content identifier for the metadata :param text: Free-form text to assign to the metadata :param extra_json: Free-form key-value pairs to assign to the metadata\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n\n\ndef log_metadata(content\n_id, text=None, extra_json=None): ““” Logs fre e-form metadata to ValidMind API.\n :param content_id: Unique co\nntent identifier for the metadata :param text: Free-fo rm text to assign to the metadata :par am extra_json: Free-form key-valu e pairs to assign to the metadata ““” metadata_dict = {\n    \"content_id\": content_id,\n    }\n\n    if text is not None:\n\n metadata_dict[\"text\"] = text\n\n   if extra_json is not None:\n        m\netadata_dict[“json”] = extra_json\n    r = api_session.post(\n\n  f\"{API_HOST}/log_metadata\",\n        data=json.dumps(\nmetadata_dict, cls=NumpyEncoder), headers={“Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could no\nt log metadata to ValidMind API”)\n      raise Exception(r.text)\n\n    prin\nt(“Successfully logged metadata”)\n    return True"
  },
  {
    "objectID": "python-library/index.html#validmind.log_metrics",
    "href": "python-library/index.html#validmind.log_metrics",
    "title": "ValidMind",
    "section": "log_metrics(metrics,run_cuid=None)",
    "text": "log_metrics(metrics,run_cuid=None)\n\nLogs metrics to ValidMind API.\n:param metrics: A list of Metric objects. :param run_cuid: The run CUID. If not provided, a new run will be created.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n\n\ndef log\n_metrics(metrics, run_cuid=None): ““”\nLogs metrics to ValidMind API.\n    :param m\netrics: A list of Metric objects. :param r un_cuid: The run CUID. If not pro vided, a new run will be created. ““” if run_cuid is None:\n       run_cuid = start_run()\n\n    serialized_metrics =\n[m.serialize() for m in metrics]\n    r = api_session.post(\n        f\"{API_HOST}/\nlog_metrics?run_cuid={run_cuid}“, data=json.dumps(seria lized_metrics, cls=NumpyEncoder), headers={”Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could n\not log metrics to ValidMind API”)\n      raise Exception(r.text)\n\n    pri\nnt(“Successfully logged metrics”)\n    return True"
  },
  {
    "objectID": "python-library/index.html#validmind.log_model",
    "href": "python-library/index.html#validmind.log_model",
    "title": "ValidMind",
    "section": "log_model(vm_model)",
    "text": "log_model(vm_model)\n\nLogs model metadata and hyperparameters to ValidMind API. :param vm_model: A ValidMind Model wrapper instance.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n\n\ndef log_model(vm_model):\n    \"\"\"\n    Logs model metadata and\nhyperparameters to ValidMind API. :param vm_model: A ValidMind Model wrapper instance. ““” r = api_session.post(\n     f\"{API_HOST}/log_model\",\n\n      data=json.dumps(vm_mode\nl.serialize(), cls=NumpyEncoder), headers={“Co ntent-Type”: “application/json”}, )\n    if r.status_code != 200:\n        print(\"Could\nnot log model to ValidMind API”)\n      raise Exception(r.text)\n\n    return True"
  },
  {
    "objectID": "python-library/index.html#validmind.log_test_results",
    "href": "python-library/index.html#validmind.log_test_results",
    "title": "ValidMind",
    "section": "log_test_results(results,run_cuid=None,dataset_type='training')",
    "text": "log_test_results(results,run_cuid=None,dataset_type='training')\n\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n:param results: A list of TestResults objects :param run_cuid: The run CUID. If not provided, a new run will be created.\nSource code in validmind/api_client.py\n\n\n\n\n\n\n\n\n\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n\n\ndef\nlog_test_results(results, run_cui d=None, dataset_type=“training”): ““” Logs test results inf ormation. This method will be cal led automatically be any function running tests but can al so be called directly if the user wants to run tests on their own.\n    :param resul\nts: A list of TestResults objects :param r un_cuid: The run CUID. If not pro vided, a new run will be created. ““” if run_cuid is None:\n       run_cuid = start_run()\n# TBD - parallelize API requests for result in results: log_test_resul t(result, run_cuid, dataset_type)\n    return True"
  },
  {
    "objectID": "python-library/404.html",
    "href": "python-library/404.html",
    "title": "ValidMind",
    "section": "",
    "text": "My Docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n My Docs\n\nWelcome to MkDocs\n\n\n\n\n\n\n\n\n\n\n\n\n404 - Not found\n\n\n\n\n\n\nMade with Material for MkDocs"
  },
  {
    "objectID": "python-library/model_validation.html#modelmetadata-objects",
    "href": "python-library/model_validation.html#modelmetadata-objects",
    "title": "ValidMind",
    "section": "ModelMetadata Objects",
    "text": "ModelMetadata Objects\n@dataclass\nclass ModelMetadata(TestContextUtils)\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nrun\ndef run()\nJust set the model to the result attribute of the test plan result and it will be logged via the log_model function"
  },
  {
    "objectID": "python-library/model_validation.html#accuracyscore-objects",
    "href": "python-library/model_validation.html#accuracyscore-objects",
    "title": "ValidMind",
    "section": "AccuracyScore Objects",
    "text": "AccuracyScore Objects\n@dataclass\nclass AccuracyScore(Metric)\nAccuracy Score"
  },
  {
    "objectID": "python-library/model_validation.html#characteristicstabilityindex-objects",
    "href": "python-library/model_validation.html#characteristicstabilityindex-objects",
    "title": "ValidMind",
    "section": "CharacteristicStabilityIndex Objects",
    "text": "CharacteristicStabilityIndex Objects\n@dataclass\nclass CharacteristicStabilityIndex(Metric)\nCharacteristic Stability Index between two datasets\n\n\nrun\ndef run()\nCalculates PSI for each of the dataset features"
  },
  {
    "objectID": "python-library/model_validation.html#confusionmatrix-objects",
    "href": "python-library/model_validation.html#confusionmatrix-objects",
    "title": "ValidMind",
    "section": "ConfusionMatrix Objects",
    "text": "ConfusionMatrix Objects\n@dataclass\nclass ConfusionMatrix(Metric)\nConfusion Matrix"
  },
  {
    "objectID": "python-library/model_validation.html#f1score-objects",
    "href": "python-library/model_validation.html#f1score-objects",
    "title": "ValidMind",
    "section": "F1Score Objects",
    "text": "F1Score Objects\n@dataclass\nclass F1Score(Metric)\nF1 Score"
  },
  {
    "objectID": "python-library/model_validation.html#permutationfeatureimportance-objects",
    "href": "python-library/model_validation.html#permutationfeatureimportance-objects",
    "title": "ValidMind",
    "section": "PermutationFeatureImportance Objects",
    "text": "PermutationFeatureImportance Objects\n@dataclass\nclass PermutationFeatureImportance(Metric)\nPermutation Feature Importance"
  },
  {
    "objectID": "python-library/model_validation.html#precisionrecallcurve-objects",
    "href": "python-library/model_validation.html#precisionrecallcurve-objects",
    "title": "ValidMind",
    "section": "PrecisionRecallCurve Objects",
    "text": "PrecisionRecallCurve Objects\n@dataclass\nclass PrecisionRecallCurve(Metric)\nPrecision Recall Curve"
  },
  {
    "objectID": "python-library/model_validation.html#precisionscore-objects",
    "href": "python-library/model_validation.html#precisionscore-objects",
    "title": "ValidMind",
    "section": "PrecisionScore Objects",
    "text": "PrecisionScore Objects\n@dataclass\nclass PrecisionScore(Metric)\nPrecision Score"
  },
  {
    "objectID": "python-library/model_validation.html#recallscore-objects",
    "href": "python-library/model_validation.html#recallscore-objects",
    "title": "ValidMind",
    "section": "RecallScore Objects",
    "text": "RecallScore Objects\n@dataclass\nclass RecallScore(Metric)\nRecall Score"
  },
  {
    "objectID": "python-library/model_validation.html#rocaucscore-objects",
    "href": "python-library/model_validation.html#rocaucscore-objects",
    "title": "ValidMind",
    "section": "ROCAUCScore Objects",
    "text": "ROCAUCScore Objects\n@dataclass\nclass ROCAUCScore(Metric)\nROC AUC Score"
  },
  {
    "objectID": "python-library/model_validation.html#roccurve-objects",
    "href": "python-library/model_validation.html#roccurve-objects",
    "title": "ValidMind",
    "section": "ROCCurve Objects",
    "text": "ROCCurve Objects\n@dataclass\nclass ROCCurve(Metric)\nROC Curve"
  },
  {
    "objectID": "python-library/model_validation.html#shapglobalimportance-objects",
    "href": "python-library/model_validation.html#shapglobalimportance-objects",
    "title": "ValidMind",
    "section": "SHAPGlobalImportance Objects",
    "text": "SHAPGlobalImportance Objects\n@dataclass\nclass SHAPGlobalImportance(TestContextUtils)\nSHAP Global Importance. Custom metric"
  },
  {
    "objectID": "python-library/model_validation.html#populationstabilityindex-objects",
    "href": "python-library/model_validation.html#populationstabilityindex-objects",
    "title": "ValidMind",
    "section": "PopulationStabilityIndex Objects",
    "text": "PopulationStabilityIndex Objects\n@dataclass\nclass PopulationStabilityIndex(Metric)\nPopulation Stability Index between two datasets"
  },
  {
    "objectID": "python-library/model_validation.html#accuracytest-objects",
    "href": "python-library/model_validation.html#accuracytest-objects",
    "title": "ValidMind",
    "section": "AccuracyTest Objects",
    "text": "AccuracyTest Objects\n@dataclass\nclass AccuracyTest(ThresholdTest)\nTest that the accuracy score is above a threshold."
  },
  {
    "objectID": "python-library/model_validation.html#f1scoretest-objects",
    "href": "python-library/model_validation.html#f1scoretest-objects",
    "title": "ValidMind",
    "section": "F1ScoreTest Objects",
    "text": "F1ScoreTest Objects\n@dataclass\nclass F1ScoreTest(ThresholdTest)\nTest that the F1 score is above a threshold."
  },
  {
    "objectID": "python-library/model_validation.html#rocaucscoretest-objects",
    "href": "python-library/model_validation.html#rocaucscoretest-objects",
    "title": "ValidMind",
    "section": "ROCAUCScoreTest Objects",
    "text": "ROCAUCScoreTest Objects\n@dataclass\nclass ROCAUCScoreTest(ThresholdTest)\nTest that the ROC AUC score is above a threshold."
  },
  {
    "objectID": "python-library/model_validation.html#trainingtestdegradationtest-objects",
    "href": "python-library/model_validation.html#trainingtestdegradationtest-objects",
    "title": "ValidMind",
    "section": "TrainingTestDegradationTest Objects",
    "text": "TrainingTestDegradationTest Objects\n@dataclass\nclass TrainingTestDegradationTest(ThresholdTest)\nTest that the training set metrics are better than the test set metrics."
  },
  {
    "objectID": "python-library/data_validation.html#datasetmetadata-objects",
    "href": "python-library/data_validation.html#datasetmetadata-objects",
    "title": "ValidMind",
    "section": "DatasetMetadata Objects",
    "text": "DatasetMetadata Objects\n@dataclass\nclass DatasetMetadata(TestContextUtils)\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadat is necessary to initialize dataset object that can be related to different metrics and test results\n\n\nrun\ndef run()\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function"
  },
  {
    "objectID": "python-library/data_validation.html#datasetcorrelations-objects",
    "href": "python-library/data_validation.html#datasetcorrelations-objects",
    "title": "ValidMind",
    "section": "DatasetCorrelations Objects",
    "text": "DatasetCorrelations Objects\n@dataclass\nclass DatasetCorrelations(Metric)\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables"
  },
  {
    "objectID": "python-library/data_validation.html#datasetdescription-objects",
    "href": "python-library/data_validation.html#datasetdescription-objects",
    "title": "ValidMind",
    "section": "DatasetDescription Objects",
    "text": "DatasetDescription Objects\n@dataclass\nclass DatasetDescription(Metric)\nCollects a set of descriptive statistics for a dataset"
  },
  {
    "objectID": "python-library/data_validation.html#classimbalancetest-objects",
    "href": "python-library/data_validation.html#classimbalancetest-objects",
    "title": "ValidMind",
    "section": "ClassImbalanceTest Objects",
    "text": "ClassImbalanceTest Objects\n@dataclass\nclass ClassImbalanceTest(ThresholdTest)\nTest that the minority class does not represent more than a threshold of the total number of examples"
  },
  {
    "objectID": "python-library/data_validation.html#duplicatestest-objects",
    "href": "python-library/data_validation.html#duplicatestest-objects",
    "title": "ValidMind",
    "section": "DuplicatesTest Objects",
    "text": "DuplicatesTest Objects\n@dataclass\nclass DuplicatesTest(ThresholdTest)\nTest that the number of duplicates is less than a threshold"
  },
  {
    "objectID": "python-library/data_validation.html#highcardinalitytest-objects",
    "href": "python-library/data_validation.html#highcardinalitytest-objects",
    "title": "ValidMind",
    "section": "HighCardinalityTest Objects",
    "text": "HighCardinalityTest Objects\n@dataclass\nclass HighCardinalityTest(ThresholdTest)\nTest that the number of unique values in a column is less than a threshold"
  },
  {
    "objectID": "python-library/data_validation.html#highpearsoncorrelationtest-objects",
    "href": "python-library/data_validation.html#highpearsoncorrelationtest-objects",
    "title": "ValidMind",
    "section": "HighPearsonCorrelationTest Objects",
    "text": "HighPearsonCorrelationTest Objects\n@dataclass\nclass HighPearsonCorrelationTest(ThresholdTest)\nTest that the Pearson correlation between two columns is less than a threshold\nInspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py"
  },
  {
    "objectID": "python-library/data_validation.html#missingvaluestest-objects",
    "href": "python-library/data_validation.html#missingvaluestest-objects",
    "title": "ValidMind",
    "section": "MissingValuesTest Objects",
    "text": "MissingValuesTest Objects\n@dataclass\nclass MissingValuesTest(ThresholdTest)\nTest that the number of missing values is less than a threshold"
  },
  {
    "objectID": "python-library/data_validation.html#skewnesstest-objects",
    "href": "python-library/data_validation.html#skewnesstest-objects",
    "title": "ValidMind",
    "section": "SkewnessTest Objects",
    "text": "SkewnessTest Objects\n@dataclass\nclass SkewnessTest(ThresholdTest)\nTest that the skewness of a column is less than a threshold"
  },
  {
    "objectID": "python-library/data_validation.html#uniquerowstest-objects",
    "href": "python-library/data_validation.html#uniquerowstest-objects",
    "title": "ValidMind",
    "section": "UniqueRowsTest Objects",
    "text": "UniqueRowsTest Objects\n@dataclass\nclass UniqueRowsTest(ThresholdTest)\nTest that the number of unique rows is greater than a threshold"
  },
  {
    "objectID": "python-library/data_validation.html#zerostest-objects",
    "href": "python-library/data_validation.html#zerostest-objects",
    "title": "ValidMind",
    "section": "ZerosTest Objects",
    "text": "ZerosTest Objects\n@dataclass\nclass ZerosTest(ThresholdTest)\nTest that the number of zeros is less than a threshold"
  },
  {
    "objectID": "python-library/vm_models.html#testplan-objects",
    "href": "python-library/vm_models.html#testplan-objects",
    "title": "ValidMind",
    "section": "TestPlan Objects",
    "text": "TestPlan Objects\n@dataclass\nclass TestPlan()\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\n\nvalidate_context\ndef validate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\n\nrun\ndef run(send=True)\nRuns the test plan"
  },
  {
    "objectID": "python-library/vm_models.html#figure-objects",
    "href": "python-library/vm_models.html#figure-objects",
    "title": "ValidMind",
    "section": "Figure Objects",
    "text": "Figure Objects\n@dataclass\nclass Figure()\nFigure objects track the schema supported by the ValidMind API\n\n\nserialize\ndef serialize()\nSerializes the Figure to a dictionary so it can be sent to the API"
  },
  {
    "objectID": "python-library/vm_models.html#modelattributes-objects",
    "href": "python-library/vm_models.html#modelattributes-objects",
    "title": "ValidMind",
    "section": "ModelAttributes Objects",
    "text": "ModelAttributes Objects\n@dataclass()\nclass ModelAttributes()\nModel attributes definition"
  },
  {
    "objectID": "python-library/vm_models.html#model-objects",
    "href": "python-library/vm_models.html#model-objects",
    "title": "ValidMind",
    "section": "Model Objects",
    "text": "Model Objects\n@dataclass\nclass Model()\nModel class wrapper\n\n\nserialize\ndef serialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\n\npredict\ndef predict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\n\nis_supported_model\n@classmethod\ndef is_supported_model(cls, model)\nChecks if the model is supported by the API"
  },
  {
    "objectID": "python-library/vm_models.html#datasettargets-objects",
    "href": "python-library/vm_models.html#datasettargets-objects",
    "title": "ValidMind",
    "section": "DatasetTargets Objects",
    "text": "DatasetTargets Objects\n@dataclass()\nclass DatasetTargets()\nDataset targets definition"
  },
  {
    "objectID": "python-library/vm_models.html#dataset-objects",
    "href": "python-library/vm_models.html#dataset-objects",
    "title": "ValidMind",
    "section": "Dataset Objects",
    "text": "Dataset Objects\n@dataclass()\nclass Dataset()\nModel class wrapper\n\n\n__post_init__\ndef __post_init__()\nSet target_column and class_labels from DatasetTargets\n\n\n\nx\n@property\ndef x()\nReturns the dataset’s features\n\n\n\ny\n@property\ndef y()\nReturns the dataset’s target column\n\n\n\nget_feature_by_id\ndef get_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\n\n\nget_feature_type\ndef get_feature_type(feature_id)\nReturns the type of the feature with the given id\n\n\n\nserialize\ndef serialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\n\ndescribe\ndef describe()\nExtracts descriptive statistics for each field in the dataset\n\n\n\nget_correlations\ndef get_correlations()\nExtracts correlations for each field in the dataset\n\n\n\nget_correlation_plots\ndef get_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\n\n\ntransformed_dataset\n@property\ndef transformed_dataset(force_refresh=False)\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns"
  },
  {
    "objectID": "python-library/vm_models.html#metric-objects",
    "href": "python-library/vm_models.html#metric-objects",
    "title": "ValidMind",
    "section": "Metric Objects",
    "text": "Metric Objects\n@dataclass\nclass Metric(TestContextUtils)\nMetric objects track the schema supported by the ValidMind API\n\n\n__post_init__\ndef __post_init__()\nSet default params if not provided\n\n\n\nrun\ndef run(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\n\ncache_results\ndef cache_results(metric_value: Union[dict, list, pd.DataFrame],\n                  figures: Optional[object] = None)\nCache the results of the metric calculation and do any post-processing if needed"
  },
  {
    "objectID": "python-library/vm_models.html#thresholdtest-objects",
    "href": "python-library/vm_models.html#thresholdtest-objects",
    "title": "ValidMind",
    "section": "ThresholdTest Objects",
    "text": "ThresholdTest Objects\n@dataclass\nclass ThresholdTest(TestContextUtils)\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\n\n__post_init__\ndef __post_init__()\nSet default params if not provided\n\n\n\nrun\ndef run(*args, **kwargs)\nRun the test and cache its results\n\n\n\ncache_results\ndef cache_results(results: List[TestResult], passed: bool)\nCache the individual results of the threshold test as a list of TestResult objects"
  },
  {
    "objectID": "python-library/vm_models.html#testplanresult-objects",
    "href": "python-library/vm_models.html#testplanresult-objects",
    "title": "ValidMind",
    "section": "TestPlanResult Objects",
    "text": "TestPlanResult Objects\n@dataclass\nclass TestPlanResult()\nResult wrapper tests that run as part of a test plan"
  },
  {
    "objectID": "python-library/vm_models.html#testcontext-objects",
    "href": "python-library/vm_models.html#testcontext-objects",
    "title": "ValidMind",
    "section": "TestContext Objects",
    "text": "TestContext Objects\n@dataclass\nclass TestContext()\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as model predictions, shared dataset metrics, etc."
  },
  {
    "objectID": "python-library/vm_models.html#testcontextutils-objects",
    "href": "python-library/vm_models.html#testcontextutils-objects",
    "title": "ValidMind",
    "section": "TestContextUtils Objects",
    "text": "TestContextUtils Objects\nclass TestContextUtils()\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\n\nclass_predictions\ndef class_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\n\n\ndf\n@property\ndef df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame"
  },
  {
    "objectID": "python-library/vm_models.html#metricresult-objects",
    "href": "python-library/vm_models.html#metricresult-objects",
    "title": "ValidMind",
    "section": "MetricResult Objects",
    "text": "MetricResult Objects\n@dataclass\nclass MetricResult()\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\n\nserialize\ndef serialize()\nSerializes the Metric to a dictionary so it can be sent to the API"
  },
  {
    "objectID": "python-library/test_plans.html#tabulardatasetdescription-objects",
    "href": "python-library/test_plans.html#tabulardatasetdescription-objects",
    "title": "ValidMind",
    "section": "TabularDatasetDescription Objects",
    "text": "TabularDatasetDescription Objects\nclass TabularDatasetDescription(TestPlan)\nTest plan to extract metadata and descriptive statistics from a tabular dataset"
  },
  {
    "objectID": "python-library/test_plans.html#tabulardataquality-objects",
    "href": "python-library/test_plans.html#tabulardataquality-objects",
    "title": "ValidMind",
    "section": "TabularDataQuality Objects",
    "text": "TabularDataQuality Objects\nclass TabularDataQuality(TestPlan)\nTest plan for data quality on tabular datasets"
  },
  {
    "objectID": "python-library/test_plans.html#tabulardataset-objects",
    "href": "python-library/test_plans.html#tabulardataset-objects",
    "title": "ValidMind",
    "section": "TabularDataset Objects",
    "text": "TabularDataset Objects\nclass TabularDataset(TestPlan)\nTest plan for generic tabular datasets"
  },
  {
    "objectID": "python-library/test_plans.html#sklearnclassifiermetrics-objects",
    "href": "python-library/test_plans.html#sklearnclassifiermetrics-objects",
    "title": "ValidMind",
    "section": "SKLearnClassifierMetrics Objects",
    "text": "SKLearnClassifierMetrics Objects\nclass SKLearnClassifierMetrics(TestPlan)\nTest plan for sklearn classifier metrics"
  },
  {
    "objectID": "python-library/test_plans.html#sklearnclassifierperformance-objects",
    "href": "python-library/test_plans.html#sklearnclassifierperformance-objects",
    "title": "ValidMind",
    "section": "SKLearnClassifierPerformance Objects",
    "text": "SKLearnClassifierPerformance Objects\nclass SKLearnClassifierPerformance(TestPlan)\nTest plan for sklearn classifier models"
  },
  {
    "objectID": "python-library/test_plans.html#sklearnclassifier-objects",
    "href": "python-library/test_plans.html#sklearnclassifier-objects",
    "title": "ValidMind",
    "section": "SKLearnClassifier Objects",
    "text": "SKLearnClassifier Objects\nclass SKLearnClassifier(TestPlan)\nTest plan for sklearn classifier models that includes both metrics and validation tests"
  },
  {
    "objectID": "python-library/index.html#installing-the-client-library",
    "href": "python-library/index.html#installing-the-client-library",
    "title": "ValidMind",
    "section": "Installing the client library",
    "text": "Installing the client library\nThe Python library can be installed with the following command:\npip install validmind"
  },
  {
    "objectID": "python-library/index.html#initializing-the-client-library",
    "href": "python-library/index.html#initializing-the-client-library",
    "title": "ValidMind",
    "section": "Initializing the client library",
    "text": "Initializing the client library\nEvery validation project has a project identifier that allows the client library to associate documentation and tests with the appropriate project. In order to initialize the client, we need to provide the following arguments:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nLocation of the ValidMind API.\n\n\napi_key\nAccount API key.\n\n\napi_secret\nAccount Secret key.\n\n\nproject\nThe project identifier.\n\n\n\nThe following code snippet shows how to initialize a ValidMind client instance:\nimport validmind as vm\n\nvm.init(\n  api_host = \"<API_HOST>\",\n  api_key = \"<API_KEY>\",\n  api_secret = \"<API_SECRET>\",\n  project = \"<PROJECT_ID>\"\n)"
  },
  {
    "objectID": "python-library/api_client.html",
    "href": "python-library/api_client.html",
    "title": "ValidMind",
    "section": "",
    "text": "validmind.api_client\nAPI Client\n\n\ninit\ndef init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\nArguments:\n\nproject str - The project CUID\napi_key str, optional - The API key. Defaults to None.\napi_secret str, optional - The API secret. Defaults to None.\napi_host str, optional - The API host. Defaults to None.\n\nRaises:\n\nValueError - If the API key and secret are not provided\n\nReturns:\n\nbool - True if the ping was successful\n\n\n\n\nlog_dataset\ndef log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\nArguments:\n\nvm_dataset validmind.VMDataset - A VM dataset object\ndataset_type str, optional - The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options dict, optional - Additional dataset options for analysis. Defaults to None.\ndataset_targets validmind.DatasetTargets, optional - A list of targets for the dataset. Defaults to None.\nfeatures list, optional - Optional. A list of features metadata. Defaults to None.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nvalidmind.VMDataset - The VMDataset object\n\n\n\n\nlog_metadata\ndef log_metadata(content_id, text=None, extra_json=None)\nLogs free-form metadata to ValidMind API.\nArguments:\n\ncontent_id str - Unique content identifier for the metadata\ntext str, optional - Free-form text to assign to the metadata. Defaults to None.\nextra_json dict, optional - Free-form key-value pairs to assign to the metadata. Defaults to None.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful\n\n\n\n\nlog_model\ndef log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API.\nArguments:\n\nvm_model validmind.VMModel - A VM model object\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful\n\n\n\n\nlog_metrics\ndef log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\nArguments:\n\nmetrics list - A list of Metric objects\nrun_cuid str, optional - The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful\n\n\n\n\nlog_test_result\ndef log_test_result(result, run_cuid=None, dataset_type=\"training\")\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\nArguments:\n\nresult validmind.TestResults - A TestResults object\nrun_cuid str, optional - The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type str, optional - The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful\n\n\n\n\nlog_test_results\ndef log_test_results(results, run_cuid=None, dataset_type=\"training\")\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\nArguments:\n\nresults list - A list of TestResults objects\nrun_cuid str, optional - The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type str, optional - The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful\n\n\n\n\nstart_run\ndef start_run()\nStarts a new test run. This method will return a test run CUID that needs to be passed to any functions logging test results to the ValidMind API.\nIf “X-RUN-CUID” was already set as an HTTP header to the session, we reuse it\n\n\n\nlog_figure\ndef log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\nArguments:\n\ndata_or_path str or matplotlib.figure.Figure - The path of the image or the data of the plot\nkey str - Identifier of the figure\nmetadata dict - Python data structure\nrun_cuid str, optional - The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises:\n\nException - If the API call fails\n\nReturns:\n\nbool - True if the API call was successful"
  },
  {
    "objectID": "python-library/client.html",
    "href": "python-library/client.html",
    "title": "ValidMind",
    "section": "",
    "text": "validmind.client\nClient interface for all data and model validation functions\n\n\ninit_dataset\ndef init_dataset(dataset,\n                 type=\"training\",\n                 options=None,\n                 targets=None,\n                 target_column=None,\n                 class_labels=None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\nArguments:\n\ndataset pd.DataFrame - We only support Pandas DataFrames at the moment\ntype str - The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions dict - A dictionary of options for the dataset\ntargets vm.vm.DatasetTargets - A list of target variables\ntarget_column str - The name of the target column in the dataset\nclass_labels dict - A list of class labels for classification problems\n\nRaises:\n\nValueError - If the dataset type is not supported\n\nReturns:\n\nvm.vm.Dataset - A VM Dataset instance\n\n\n\n\ninit_model\ndef init_model(model)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\nArguments:\n\nmodel - A trained sklearn model\n\nRaises:\n\nValueError - If the model type is not supported\n\nReturns:\n\nvm.vm.Model - A VM Model instance\n\n\n\n\nrun_test_plan\ndef run_test_plan(test_plan_name, send=True, **kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\nArguments:\n\ntest_plan_name str - The test plan name (e.g. ‘sklearn_classifier’)\nsend bool, optional - Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs - Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises:\n\nValueError - If the test plan name is not found or if there is an error initializing the test plan\n\nReturns:\n\ndict - A dictionary of test results"
  },
  {
    "objectID": "python-library/test.html",
    "href": "python-library/test.html",
    "title": "ValidMind",
    "section": "",
    "text": "validmind.init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and /pings the API to ensure the provided credentials are valid.\nThis is an example to test how to show the code in the documentation.\nimport validmind as vm\n\nvm.init(\n    project=\"my-project\",\n    api_key=\"my-api-key\",\n    api_secret\n)\n\nParameters\n\nproject (str) – The project cuid.\napi_key (str) – The API key.\napi_secret (str) – The API secret.\napi_host (str) – The API host.\n\nRaises\nValueError – If the API key and secret are not provided.\nReturns\nTrue if the ping was successful.\nReturn type\nbool\n\n\n\n\nvalidmind.init_dataset(dataset, type='training', options=None, targets=None, target_column=None, class_labels=None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\n\n\n\n\nvalidmind.init_model(model)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained model instance\n\n\n\n\nvalidmind.log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\ndataset – A VM dataset object\ndataset_type – The type of dataset. Can be one of “training”, “test”, or “validation”.\ndataset_options – Additional dataset options for analysis\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset.\nfeatures – Optional. A list of features metadata.\n\n\n\n\n\nvalidmind.log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\n\nParameters\n\ndata_or_path – the path of the image or the data of the plot\nkey – identifier of the figure\nmetadata – python data structure\nrun_cuid – run cuid from start_run\n\n\n\n\n\nvalidmind.log_metadata(content_id, text=None, extra_json=None)\nLogs free-form metadata to ValidMind API.\n\nParameters\n\ncontent_id – Unique content identifier for the metadata\ntext – Free-form text to assign to the metadata\nextra_json – Free-form key-value pairs to assign to the metadata\n\n\n\n\n\nvalidmind.log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics – A list of Metric objects.\nrun_cuid – The run CUID. If not provided, a new run will be created.\n\n\n\n\n\nvalidmind.log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API. :param vm_model: A ValidMind Model wrapper instance.\n\n\n\nvalidmind.log_test_results(results, run_cuid=None, dataset_type='training')\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults – A list of TestResults objects\nrun_cuid – The run CUID. If not provided, a new run will be created.\n\n\n\n\n\nvalidmind.run_test_plan(test_plan_name, send=True, \\*\\*kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘sklearn_classifier’)\nsend (bool) – Whether to post the test results to the API. send=False is useful for testing\nkwargs (dict) – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details."
  },
  {
    "objectID": "guide/getstartedvalidator.html",
    "href": "guide/getstartedvalidator.html",
    "title": "Get started as a validator",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "guide/about.html",
    "href": "guide/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "guide/getstarteddeveloper.html",
    "href": "guide/getstarteddeveloper.html",
    "title": "Get started as a developer",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "guide/getstartedauditor.html",
    "href": "guide/getstartedauditor.html",
    "title": "Get started as an auditor",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]